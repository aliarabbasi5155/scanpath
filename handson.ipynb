{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fork_000000537944.jpg': [56, 229, 33, 49],\n",
       " 'knife_000000233539.jpg': [372, 193, 114, 92],\n",
       " 'tv_000000192651.jpg': [51, 138, 60, 54],\n",
       " 'laptop_000000335308.jpg': [16, 210, 52, 39],\n",
       " 'sink_000000164725.jpg': [158, 219, 159, 94],\n",
       " 'fork_000000061672.jpg': [383, 101, 51, 141],\n",
       " 'laptop_000000204979.jpg': [48, 98, 130, 101],\n",
       " 'mouse_000000539056.jpg': [386, 206, 41, 44],\n",
       " 'fork_000000323370.jpg': [399, 55, 65, 184],\n",
       " 'tv_000000203160.jpg': [98, 57, 71, 71]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "bbox_annos = np.load(\"./files/Google Drive/bbox_annos.npy\", allow_pickle=True).item()\n",
    "bbox_annos_sample = {key: bbox_annos[key] for key in list(bbox_annos.keys())[:10]}\n",
    "bbox_annos_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "bbox_annos = np.load(\"./files/Google Drive/bbox_annos.npy\", allow_pickle=True).item()\n",
    "bbox_annos_sample = {key: bbox_annos[key] for key in list(bbox_annos.keys())[:10]}\n",
    "bbox_annos_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been split and written to the respective files.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Replace 'data.json' with the path to your original JSON file\n",
    "with open('files/DCBs_JSONs/dataset_test/SAIL_fixations_TP_train.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Shuffle the data to ensure random splitting\n",
    "random.shuffle(data)\n",
    "\n",
    "# Calculate the split index for 80/20 split\n",
    "split_index = int(len(data) * 0.8)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data = data[:split_index]\n",
    "valid_data = data[split_index:]\n",
    "\n",
    "# Write the training data to 'human_scanpath_train_split.json'\n",
    "with open('files/human_scanpath_train_split.json', 'w') as train_file:\n",
    "    json.dump(train_data, train_file, indent=4)\n",
    "\n",
    "# Write the validation data to 'human_scanpath_valid_split.json'\n",
    "with open('files/human_scanpath_valid_split.json', 'w') as valid_file:\n",
    "    json.dump(valid_data, valid_file, indent=4)\n",
    "\n",
    "print(\"Data has been split and written to the respective files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries checked: 267\n",
      "Entries with values exceeding thresholds: 10\n",
      "\n",
      "Entries exceeding thresholds:\n",
      "\n",
      "Entry Index: 60\n",
      "Name: Slide3.jpg\n",
      "Y values exceeding 320: [749.9488]\n",
      "\n",
      "Entry Index: 62\n",
      "Name: Slide7.jpg\n",
      "Y values exceeding 320: [606.6528]\n",
      "\n",
      "Entry Index: 65\n",
      "Name: Slide13.jpg\n",
      "Y values exceeding 320: [934.8512]\n",
      "\n",
      "Entry Index: 72\n",
      "Name: Slide33.jpg\n",
      "Y values exceeding 320: [324.1504]\n",
      "\n",
      "Entry Index: 120\n",
      "Name: Slide29.jpg\n",
      "Y values exceeding 320: [324.4416]\n",
      "\n",
      "Entry Index: 227\n",
      "Name: Slide3.jpg\n",
      "Y values exceeding 320: [344.2976]\n",
      "\n",
      "Entry Index: 230\n",
      "Name: Slide11.jpg\n",
      "Y values exceeding 320: [354.6464]\n",
      "\n",
      "Entry Index: 231\n",
      "Name: Slide13.jpg\n",
      "Y values exceeding 320: [366.288]\n",
      "\n",
      "Entry Index: 232\n",
      "Name: Slide15.jpg\n",
      "Y values exceeding 320: [333.8688]\n",
      "\n",
      "Entry Index: 243\n",
      "Name: Slide13.jpg\n",
      "Y values exceeding 320: [331.4528]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "json_file_path = 'files/DCBs_JSONs/dataset_test/SAIL_fixations_TP_train.json'\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(json_file_path):\n",
    "    print(f\"File not found: {json_file_path}\")\n",
    "    exit(1)\n",
    "\n",
    "# Load the JSON data\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Initialize counters and lists to store entries exceeding thresholds\n",
    "entries_exceeding_threshold = []\n",
    "total_entries = len(data)\n",
    "entries_with_exceeding_values = 0\n",
    "\n",
    "# Thresholds\n",
    "x_threshold = 512\n",
    "y_threshold = 320\n",
    "\n",
    "# Iterate over each entry in the data\n",
    "for idx, entry in enumerate(data):\n",
    "    X_values = entry.get('X', [])\n",
    "    Y_values = entry.get('Y', [])\n",
    "    entry_name = entry.get('name', f'Entry {idx}')\n",
    "\n",
    "    # Flags to check if any value exceeds the thresholds\n",
    "    x_exceeds = any(x > x_threshold for x in X_values)\n",
    "    y_exceeds = any(y > y_threshold for y in Y_values)\n",
    "\n",
    "    # If any value exceeds the thresholds, store the entry information\n",
    "    if x_exceeds or y_exceeds:\n",
    "        entries_with_exceeding_values += 1\n",
    "        entries_exceeding_threshold.append({\n",
    "            'entry_index': idx,\n",
    "            'name': entry_name,\n",
    "            'X_exceeds': [x for x in X_values if x > x_threshold],\n",
    "            'Y_exceeds': [y for y in Y_values if y > y_threshold]\n",
    "        })\n",
    "\n",
    "# Reporting the results\n",
    "print(f\"Total entries checked: {total_entries}\")\n",
    "print(f\"Entries with values exceeding thresholds: {entries_with_exceeding_values}\\n\")\n",
    "\n",
    "if entries_exceeding_threshold:\n",
    "    print(\"Entries exceeding thresholds:\")\n",
    "    for item in entries_exceeding_threshold:\n",
    "        print(f\"\\nEntry Index: {item['entry_index']}\")\n",
    "        print(f\"Name: {item['name']}\")\n",
    "        if item['X_exceeds']:\n",
    "            print(f\"X values exceeding {x_threshold}: {item['X_exceeds']}\")\n",
    "        if item['Y_exceeds']:\n",
    "            print(f\"Y values exceeding {y_threshold}: {item['Y_exceeds']}\")\n",
    "else:\n",
    "    print(\"No entries have values exceeding the specified thresholds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been filtered, split, and written to the respective files.\n",
      "Total entries after filtering: 257 and 10 entries were removed.\n",
      "Training entries: 205\n",
      "Validation entries: 52\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "json_file_path = 'DCBs_JSONs/dataset_test/SAIL_fixations_TA_train.json'\n",
    "\n",
    "# Output file paths\n",
    "train_file_path = 'human_scanpath_train_split.json'\n",
    "valid_file_path = 'human_scanpath_valid_split.json'\n",
    "\n",
    "# Check if the input file exists\n",
    "if not os.path.exists(json_file_path):\n",
    "    print(f\"File not found: {json_file_path}\")\n",
    "    exit(1)\n",
    "\n",
    "# Load the JSON data\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Thresholds\n",
    "x_threshold = 512\n",
    "y_threshold = 320\n",
    "\n",
    "# Filter entries that have no X[i] > 512 and no Y[i] > 320\n",
    "filtered_data = []\n",
    "for idx, entry in enumerate(data):\n",
    "    X_values = entry.get('X', [])\n",
    "    Y_values = entry.get('Y', [])\n",
    "\n",
    "    # Check if any X[i] > 512 or Y[i] > 320\n",
    "    x_exceeds = any(x > x_threshold for x in X_values)\n",
    "    y_exceeds = any(y > y_threshold for y in Y_values)\n",
    "\n",
    "    if not x_exceeds and not y_exceeds:\n",
    "        filtered_data.append(entry)\n",
    "\n",
    "# Shuffle the filtered data\n",
    "random.shuffle(filtered_data)\n",
    "\n",
    "# Calculate the split index for 80/20 split\n",
    "split_index = int(len(filtered_data) * 0.8)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data = filtered_data[:split_index]\n",
    "valid_data = filtered_data[split_index:]\n",
    "\n",
    "# Update the 'split' field in each entry\n",
    "for entry in train_data:\n",
    "    entry['split'] = 'train'\n",
    "\n",
    "for entry in valid_data:\n",
    "    entry['split'] = 'valid'\n",
    "\n",
    "# Write the training data to 'human_scanpath_train_split.json'\n",
    "with open(train_file_path, 'w') as train_file:\n",
    "    json.dump(train_data, train_file, indent=4)\n",
    "\n",
    "# Write the validation data to 'human_scanpath_valid_split.json'\n",
    "with open(valid_file_path, 'w') as valid_file:\n",
    "    json.dump(valid_data, valid_file, indent=4)\n",
    "\n",
    "print(\"Data has been filtered, split, and written to the respective files.\")\n",
    "print(f\"Total entries after filtering: {len(filtered_data)} and {len(data) - len(filtered_data)} entries were removed.\")\n",
    "print(f\"Training entries: {len(train_data)}\")\n",
    "print(f\"Validation entries: {len(valid_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been filtered, split, and written to the respective files.\n",
      "Total entries after filtering: 142 and 56 entries were removed.\n",
      "Training entries: 113\n",
      "Validation entries: 29\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "json_file_path = 'fixation_data.json'\n",
    "\n",
    "# Output file paths\n",
    "train_file_path = 'stroop_human_scanpath_train_split.json'\n",
    "valid_file_path = 'stroop_human_scanpath_valid_split.json'\n",
    "\n",
    "# Check if the input file exists\n",
    "if not os.path.exists(json_file_path):\n",
    "    print(f\"File not found: {json_file_path}\")\n",
    "    exit(1)\n",
    "\n",
    "# Load the JSON data\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Add 'featureA', 'featureB', and 'featureC' to each entry\n",
    "for entry in data:\n",
    "    # length = entry.get('length', 0)\n",
    "    length = 3\n",
    "    # Ensure length is an integer and greater than 0\n",
    "    if isinstance(length, int) and length > 0:\n",
    "        # Generate random values between 0 and 1\n",
    "        entry['eeg_data'] = [random.random() for _ in range(length)]\n",
    "    else:\n",
    "        # Handle entries without valid 'length'\n",
    "        entry['eeg_data'] = []\n",
    "\n",
    "# Thresholds\n",
    "x_threshold = 512\n",
    "y_threshold = 320\n",
    "\n",
    "# Filter entries that have no X[i] > 512 and no Y[i] > 320\n",
    "filtered_data = []\n",
    "for idx, entry in enumerate(data):\n",
    "    X_values = entry.get('X', [])\n",
    "    Y_values = entry.get('Y', [])\n",
    "\n",
    "    # Check if any X[i] > 512 or Y[i] > 320\n",
    "    x_exceeds = any(x > x_threshold for x in X_values)\n",
    "    y_exceeds = any(y > y_threshold for y in Y_values)\n",
    "\n",
    "    if not x_exceeds and not y_exceeds:\n",
    "        filtered_data.append(entry)\n",
    "\n",
    "# Shuffle the filtered data\n",
    "random.shuffle(filtered_data)\n",
    "\n",
    "# Calculate the split index for 80/20 split\n",
    "split_index = int(len(filtered_data) * 0.8)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data = filtered_data[:split_index]\n",
    "valid_data = filtered_data[split_index:]\n",
    "\n",
    "# Update the 'split' field in each entry\n",
    "for entry in train_data:\n",
    "    entry['split'] = 'train'\n",
    "\n",
    "for entry in valid_data:\n",
    "    entry['split'] = 'valid'\n",
    "\n",
    "# Write the training data to 'human_scanpath_train_split.json'\n",
    "with open(train_file_path, 'w') as train_file:\n",
    "    json.dump(train_data, train_file, indent=4)\n",
    "\n",
    "# Write the validation data to 'human_scanpath_valid_split.json'\n",
    "with open(valid_file_path, 'w') as valid_file:\n",
    "    json.dump(valid_data, valid_file, indent=4)\n",
    "\n",
    "print(\"Data has been filtered, split, and written to the respective files.\")\n",
    "print(f\"Total entries after filtering: {len(filtered_data)} and {len(data) - len(filtered_data)} entries were removed.\")\n",
    "print(f\"Training entries: {len(train_data)}\")\n",
    "print(f\"Validation entries: {len(valid_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x < 512 and y < 320\n",
      "subject: 1 name: Slide9.jpg X: [238.41, 214.29, 276.73, 159.58, 270.98, 277.36, 291.09, 257.26] Y: [163.16, 149.78, 247.41, 345.88, 283.25, 311.04, 286.31, 271.75]\n",
      "subject: 1 name: Slide13.jpg X: [279.8, 266.83, 252.45, 283.39, 199.78, 273.94, 279.36, 275.11, 290.16] Y: [282.42, 291.17, 306.74, 278.08, 324.68, 272.32, 285.47, 285.31, 259.36]\n",
      "subject: 1 name: Slide15.jpg X: [285.34, 275.81, 171.84, 261.91, 248.95, 269.29, 259.89, 242.81] Y: [219.86, 276.07, 336.4, 279.17, 267.66, 271.41, 256.1, 260.26]\n",
      "subject: 1 name: Slide17.jpg X: [266.54, 296.43, 245.89, 275.13, 305.83, 277.39, 274.51, 258.03, 289.46] Y: [192.48, 156.49, 188.66, 260.68, 372.41, 254.01, 245.97, 333.4, 286.3]\n",
      "subject: 1 name: Slide19.jpg X: [269.73, 265.4, 252.17, 241.56, 296.82, 119.9, 258.4, 335.92, 251.02, 257.93] Y: [255.78, 257.6, 187.42, 138.34, 271.8, 486.44, 262.1, 344.14, 264.73, 326.36]\n",
      "subject: 1 name: Slide23.jpg X: [211.21, 270.88, 265.54, 276.58, 265.87, 246.27, 262.59, 277.87, 316.6] Y: [114.52, 149.33, 249.3, 279.0, 253.03, 337.64, 277.61, 281.35, 380.96]\n",
      "subject: 1 name: Slide27.jpg X: [319.43, 242.65, 285.2, 258.66, 257.42, 284.24, 274.45, 228.81, 254.38] Y: [235.34, 209.27, 232.46, 276.9, 288.21, 251.75, 231.45, 333.95, 291.01]\n",
      "subject: 1 name: Slide31.jpg X: [240.17, 252.17, 283.28, 400.1, 254.82, 247.3, 259.81] Y: [176.92, 155.73, 259.33, 438.04, 279.01, 270.49, 356.85]\n",
      "subject: 1 name: Slide35.jpg X: [264.01, 261.71, 260.12, 274.66, 324.94, 277.07, 261.62, 293.82, 284.56, 293.68] Y: [89.97, 9.16, 176.51, 306.74, 354.16, 282.42, 263.46, 280.78, 267.31, 283.84]\n",
      "subject: 1 name: Slide37.jpg X: [280.33, 269.74, 278.3, 294.19, 287.49, 209.5, 296.19, 289.72] Y: [313.87, 176.25, 272.49, 297.83, 266.62, 336.14, 283.62, 332.79]\n",
      "subject: 2 name: Slide3.jpg X: [281.3, 256.16, 267.37, 250.64, 234.88, 247.34, 242.08] Y: [150.02, 279.3, 273.88, 281.3, 346.73, 355.01, 303.02]\n",
      "subject: 2 name: Slide19.jpg X: [246.43, 258.38, 274.86, 275.42, 262.11, 269.35] Y: [133.56, 74.63, 173.31, 244.6, 381.19, 304.56]\n",
      "subject: 2 name: Slide27.jpg X: [218.76, 274.86, 260.03, 253.86, 289.84] Y: [185.0, 247.29, 277.24, 302.44, 351.25]\n",
      "subject: 2 name: Slide29.jpg X: [287.5, 288.85, 265.94, 260.7, 243.71, 253.7, 242.55, 252.98, 245.61, 249.51] Y: [249.65, 420.71, 251.78, 135.96, 10.56, 0, 18.86, 28.71, 99.0, 4.14]\n",
      "subject: 3 name: Slide15.jpg X: [274.01, 279.87, 299.82] Y: [257.93, 284.4, 342.21]\n",
      "subject: 3 name: Slide17.jpg X: [269.71, 274.86, 315.86] Y: [299.38, 276.77, 321.43]\n",
      "subject: 3 name: Slide27.jpg X: [231.64, 284.15, 277.0, 285.08, 288.54, 314.69] Y: [182.14, 34.34, 39.7, 288.77, 275.02, 339.08]\n",
      "subject: 3 name: Slide31.jpg X: [253.78, 260.34, 246.99, 321.97, 270.88, 282.54] Y: [145.18, 158.06, 473.72, 341.02, 277.67, 319.15]\n",
      "subject: 3 name: Slide37.jpg X: [0, 275.81, 268.43, 344.84] Y: [602.76, 243.55, 295.85, 328.99]\n",
      "subject: 4 name: Slide1.jpg X: [222.82, 241.05, 208.27, 212.07, 94.3, 250.64, 250.82, 247.15] Y: [169.72, 215.41, 231.13, 276.78, 369.75, 332.05, 297.39, 305.33]\n",
      "subject: 4 name: Slide3.jpg X: [243.21, 249.44, 228.66, 232.54, 234.88, 256.59, 244.37, 210.51] Y: [138.42, 139.44, 257.02, 303.02, 228.24, 305.54, 294.74, 320.15]\n",
      "subject: 4 name: Slide13.jpg X: [244.39, 239.52, 218.93, 246.36, 239.89, 234.92, 377.33, 219.56, 233.19, 216.31] Y: [132.76, 167.97, 239.08, 251.81, 382.49, 306.1, 365.94, 303.64, 326.46, 406.43]\n",
      "subject: 4 name: Slide27.jpg X: [248.61, 275.88, 304.73, 266.11, 261.62, 244.5, 258.91, 262.77, 273.87, 253.53, 239.06] Y: [146.42, 222.0, 0, 290.52, 323.39, 334.35, 295.7, 285.08, 317.93, 286.48, 358.39]\n",
      "subject: 4 name: Slide31.jpg X: [240.88, 257.75, 255.77, 252.51, 252.5, 245.18, 266.94, 272.91, 293.68] Y: [227.98, 247.21, 317.28, 292.72, 297.68, 273.13, 265.19, 302.94, 325.09]\n",
      "subject: 4 name: Slide35.jpg X: [297.06, 244.75, 225.36, 248.76, 238.89, 281.0, 211.03, 259.81, 225.08, 265.06] Y: [164.06, 130.17, 151.05, 283.61, 270.2, 230.91, 293.07, 289.18, 296.9, 352.79]\n",
      "subject: 4 name: Slide37.jpg X: [242.11, 240.62, 247.94, 256.68, 233.2, 249.81, 236.45, 64.57, 94.64, 253.77, 266.5, 266.95] Y: [158.65, 133.84, 220.99, 287.36, 287.48, 304.83, 278.73, 386.35, 386.66, 274.93, 288.29, 301.69]\n",
      "subject: 5 name: Slide31.jpg X: [238.28, 210.32, 224.23, 259.18, 251.34, 249.57, 256.23, 261.73, 256.5, 267.25, 273.23, 272.53, 274.09, 283.44] Y: [111.78, 80.63, 136.83, 167.36, 144.85, 263.32, 276.45, 267.86, 308.57, 322.47, 277.13, 297.98, 256.45, 268.46]\n",
      "subject: 5 name: Slide39.jpg X: [255.33, 239.37, 222.33, 64.73, 200.64, 259.52, 265.04, 286.53, 414.91, 334.07, 175.4, 236.23, 229.36, 0] Y: [78.39, 205.65, 116.48, 100.55, 124.09, 104.26, 134.31, 48.67, 63.84, 26.45, 159.05, 167.86, 192.29, 365.55]\n",
      "subject: 6 name: Slide3.jpg X: [269.46, 276.34, 294.0, 267.03, 273.88, 293.71, 262.25, 280.76, 270.02] Y: [138.69, 271.65, 297.96, 310.17, 369.79, 360.88, 279.44, 289.01, 221.8]\n",
      "subject: 6 name: Slide9.jpg X: [271.51, 261.55, 280.92, 298.96, 280.66, 281.04, 268.07, 283.18, 277.7] Y: [105.67, 156.39, 252.61, 293.34, 300.65, 285.55, 318.79, 326.79, 303.35]\n",
      "subject: 6 name: Slide15.jpg X: [255.64, 284.99, 270.86, 269.64, 279.73, 306.81, 289.47, 278.9] Y: [275.7, 128.56, 179.96, 295.35, 254.22, 290.87, 358.8, 271.79]\n",
      "subject: 6 name: Slide23.jpg X: [265.25, 268.84, 249.92, 281.03, 276.98, 261.16, 274.53, 281.45] Y: [151.45, 214.97, 131.28, 275.71, 320.97, 274.96, 298.3, 280.75]\n",
      "subject: 8 name: Slide15.jpg X: [305.79, 298.56, 280.36, 232.91, 297.98, 257.26, 277.55, 268.54, 284.56, 269.81, 297.57] Y: [274.56, 366.91, 148.53, 115.1, 145.0, 113.77, 0, 177.13, 281.81, 300.5, 254.02]\n",
      "subject: 8 name: Slide19.jpg X: [193.09, 280.01, 214.72, 292.29, 289.01, 259.58, 303.4, 256.79, 315.55, 315.79, 267.43] Y: [231.67, 142.4, 114.51, 176.84, 283.71, 282.26, 265.56, 269.89, 283.92, 332.44, 232.28]\n",
      "subject: 8 name: Slide21.jpg X: [577.52, 241.41, 247.0, 256.07, 266.4, 243.95, 276.6, 274.07, 285.67, 272.28, 284.94, 269.46, 262.0] Y: [515.68, 146.64, 133.6, 99.65, 14.54, 122.41, 7.84, 35.62, 0, 2.52, 29.5, 0, 9.09]\n",
      "subject: 8 name: Slide25.jpg X: [149.98, 265.99, 262.91, 298.53, 289.84, 244.06, 272.62, 273.89] Y: [345.97, 67.5, 0, 22.59, 181.6, 0, 11.64, 0]\n",
      "subject: 8 name: Slide27.jpg X: [267.67, 244.35, 274.53, 264.24, 259.39, 279.04, 291.68, 290.17, 258.86] Y: [107.02, 115.33, 125.73, 224.09, 272.92, 285.46, 299.2, 273.4, 384.35]\n",
      "subject: 8 name: Slide29.jpg X: [304.94, 390.24, 343.04, 257.95, 289.81, 263.13, 266.71, 258.53, 234.72, 318.65, 279.6, 273.54, 267.59, 240.65, 276.08] Y: [325.58, 386.11, 214.34, 149.07, 122.81, 3.54, 0, 0, 79.32, 108.87, 6.13, 11.92, 0, 30.8, 0]\n",
      "subject: 8 name: Slide31.jpg X: [194.39, 250.82, 255.94, 280.82, 273.91, 281.52, 275.99, 283.96, 284.43, 268.32] Y: [166.4, 133.97, 197.07, 287.16, 282.68, 290.99, 260.44, 280.08, 409.43, 297.27]\n",
      "subject: 10 name: Slide3.jpg X: [287.03, 274.17, 258.51, 243.66, 260.1, 233.63, 247.07, 264.32, 253.49, 257.22] Y: [286.62, 228.32, 257.16, 308.93, 372.08, 327.59, 322.21, 354.87, 311.06, 325.3]\n",
      "subject: 10 name: Slide9.jpg X: [279.59, 266.75, 256.44, 266.41, 250.78, 270.61, 270.67, 225.23, 231.8, 263.59, 243.92, 277.25] Y: [246.79, 222.0, 363.24, 457.37, 374.1, 390.69, 356.97, 410.59, 287.52, 303.08, 266.1, 388.88]\n",
      "subject: 10 name: Slide13.jpg X: [247.64, 257.13, 247.28, 264.68, 254.73, 256.91, 257.1, 259.71, 235.13, 262.66, 246.44] Y: [288.19, 248.64, 341.11, 312.05, 359.71, 263.58, 368.84, 339.56, 280.04, 309.17, 325.83]\n",
      "subject: 10 name: Slide15.jpg X: [271.38, 240.48, 253.57, 245.5, 250.05, 283.77, 262.27, 249.21, 275.95, 238.37, 308.41] Y: [266.47, 229.64, 254.44, 241.19, 77.09, 356.7, 340.75, 287.42, 285.39, 302.99, 402.44]\n",
      "subject: 10 name: Slide17.jpg X: [264.29, 247.38, 302.18, 225.99, 244.72, 237.39, 247.74, 260.04, 267.68, 246.47] Y: [244.72, 296.82, 417.31, 331.15, 351.14, 271.62, 254.43, 338.89, 343.57, 277.65]\n",
      "subject: 10 name: Slide19.jpg X: [294.07, 286.89, 281.75, 315.76, 292.3, 261.52, 265.59, 304.38] Y: [231.23, 411.8, 334.55, 389.91, 367.41, 367.65, 285.42, 355.64]\n",
      "subject: 10 name: Slide23.jpg X: [264.63, 259.73, 246.61, 245.25, 244.38, 263.61, 274.66, 256.33, 256.69, 253.61, 276.75, 255.06] Y: [229.85, 262.4, 275.78, 312.61, 323.05, 363.52, 284.99, 297.98, 342.2, 277.03, 334.84, 243.05]\n",
      "subject: 10 name: Slide27.jpg X: [674.44, 702.92, 242.64, 244.12, 256.24, 249.98, 268.1, 257.46, 229.73, 256.67] Y: [1601.12, 1610.87, 284.6, 317.45, 342.6, 325.3, 325.13, 339.81, 373.21, 333.5]\n",
      "subject: 10 name: Slide29.jpg X: [256.2, 242.99, 244.19, 263.87, 248.88, 255.06, 261.93, 254.94, 241.54, 254.22, 249.24, 253.62, 259.43, 230.83, 225.55] Y: [243.41, 294.76, 322.39, 203.25, 229.51, 257.16, 63.2, 24.86, 36.31, 13.08, 80.88, 0, 31.98, 11.87, 75.22]\n",
      "subject: 10 name: Slide31.jpg X: [250.0, 252.86, 267.87, 256.77, 217.31, 249.95, 233.9, 266.02, 262.94, 275.69, 260.08, 260.67, 267.83] Y: [250.01, 227.77, 178.45, 285.11, 85.79, 331.3, 297.68, 285.48, 314.38, 347.65, 352.17, 387.58, 308.66]\n",
      "subject: 10 name: Slide35.jpg X: [253.81, 256.03, 225.01, 257.31, 228.75, 258.36, 263.33, 277.54, 308.72, 300.94, 254.53, 247.48, 251.44, 275.13] Y: [219.04, 266.68, 333.1, 348.54, 314.46, 387.71, 321.91, 403.29, 450.32, 360.29, 338.79, 295.2, 327.99, 430.16]\n",
      "subject: 10 name: Slide37.jpg X: [270.99, 288.42, 258.91, 242.75, 260.47, 223.72, 248.76, 257.81, 581.93, 320.13] Y: [260.19, 317.74, 276.82, 223.08, 360.99, 320.74, 290.68, 263.7, 440.14, 503.84]\n",
      "subject: 11 name: Slide13.jpg X: [242.18, 282.29, 236.95, 265.6, 272.81, 271.1, 241.45, 244.84] Y: [148.1, 164.44, 145.1, 315.57, 283.56, 373.21, 232.68, 260.62]\n",
      "subject: 11 name: Slide15.jpg X: [218.0, 266.93, 219.75, 228.94, 250.78, 294.21, 255.78, 227.13, 218.42] Y: [166.33, 152.45, 166.09, 31.36, 68.6, 405.79, 278.92, 285.86, 158.27]\n",
      "subject: 11 name: Slide17.jpg X: [233.5, 260.58, 264.16, 224.22, 241.6, 249.67, 220.05] Y: [153.58, 176.39, 330.41, 291.16, 284.71, 231.07, 171.58]\n",
      "subject: 11 name: Slide19.jpg X: [242.79, 241.16, 258.48, 227.37, 271.81, 263.15, 244.99] Y: [163.52, 210.01, 301.17, 348.19, 283.21, 308.06, 301.39]\n",
      "subject: 11 name: Slide37.jpg X: [225.29, 246.74, 238.32, 271.53, 242.4] Y: [122.2, 323.05, 326.84, 286.39, 369.12]\n",
      "Subject 1 is missing the following slides: Slide41.jpg, Slide43.jpg, Slide45.jpg, Slide47.jpg, Slide49.jpg, Slide51.jpg, Slide53.jpg, Slide55.jpg, Slide57.jpg, Slide59.jpg\n",
      "Subject 2 is missing the following slides: Slide41.jpg, Slide43.jpg, Slide45.jpg, Slide47.jpg, Slide49.jpg, Slide51.jpg, Slide53.jpg, Slide55.jpg, Slide57.jpg, Slide59.jpg\n",
      "Subject 3 is missing the following slides: Slide19.jpg, Slide21.jpg, Slide41.jpg, Slide43.jpg, Slide45.jpg, Slide47.jpg, Slide49.jpg, Slide51.jpg, Slide53.jpg, Slide55.jpg, Slide57.jpg, Slide59.jpg\n",
      "Subject 4 is missing the following slides: Slide41.jpg, Slide43.jpg, Slide45.jpg, Slide47.jpg, Slide49.jpg, Slide51.jpg, Slide53.jpg, Slide55.jpg, Slide57.jpg, Slide59.jpg\n",
      "Subject 5 is missing the following slides: Slide41.jpg, Slide43.jpg, Slide45.jpg, Slide47.jpg, Slide49.jpg, Slide51.jpg, Slide53.jpg, Slide55.jpg, Slide57.jpg, Slide59.jpg\n",
      "Subject 6 is missing the following slides: Slide41.jpg, Slide43.jpg, Slide45.jpg, Slide47.jpg, Slide49.jpg, Slide51.jpg, Slide53.jpg, Slide55.jpg, Slide57.jpg, Slide59.jpg\n",
      "Subject 7 is missing the following slides: Slide1.jpg, Slide3.jpg, Slide5.jpg, Slide7.jpg, Slide9.jpg, Slide11.jpg, Slide13.jpg, Slide15.jpg, Slide17.jpg, Slide19.jpg, Slide21.jpg, Slide23.jpg, Slide25.jpg, Slide27.jpg, Slide29.jpg, Slide31.jpg, Slide33.jpg, Slide35.jpg, Slide37.jpg, Slide39.jpg, Slide41.jpg, Slide43.jpg, Slide45.jpg, Slide47.jpg, Slide49.jpg, Slide51.jpg, Slide53.jpg, Slide55.jpg, Slide57.jpg, Slide59.jpg\n",
      "Subject 8 is missing the following slides: Slide41.jpg, Slide43.jpg, Slide45.jpg, Slide47.jpg, Slide49.jpg, Slide51.jpg, Slide53.jpg, Slide55.jpg, Slide57.jpg, Slide59.jpg\n",
      "Subject 9 is missing the following slides: Slide41.jpg, Slide43.jpg, Slide45.jpg, Slide47.jpg, Slide49.jpg, Slide51.jpg, Slide53.jpg, Slide55.jpg, Slide57.jpg, Slide59.jpg\n",
      "Subject 10 is missing the following slides: Slide41.jpg, Slide43.jpg, Slide45.jpg, Slide47.jpg, Slide49.jpg, Slide51.jpg, Slide53.jpg, Slide55.jpg, Slide57.jpg, Slide59.jpg\n",
      "Subject 11 is missing the following slides: Slide41.jpg, Slide43.jpg, Slide45.jpg, Slide47.jpg, Slide49.jpg, Slide51.jpg, Slide53.jpg, Slide55.jpg, Slide57.jpg, Slide59.jpg\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "json_file_path = 'fixation_data.json'\n",
    "\n",
    "# Output file paths\n",
    "train_file_path = 'stroop_human_scanpath_train_split.json'\n",
    "valid_file_path = 'stroop_human_scanpath_valid_split.json'\n",
    "\n",
    "# Check if the input file exists\n",
    "if not os.path.exists(json_file_path):\n",
    "    print(f\"File not found: {json_file_path}\")\n",
    "    exit(1)\n",
    "\n",
    "# Load the JSON data\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Thresholds\n",
    "x_threshold = 512\n",
    "y_threshold = 320\n",
    "\n",
    "# Filter entries that have no X[i] > 512 and no Y[i] > 320\n",
    "filtered_data = []\n",
    "removed_entries = []\n",
    "for idx, entry in enumerate(data):\n",
    "    X_values = entry.get('X', [])\n",
    "    Y_values = entry.get('Y', [])\n",
    "\n",
    "    # Check if any X[i] > 512 or Y[i] > 320\n",
    "    x_exceeds = any(x > x_threshold for x in X_values)\n",
    "    y_exceeds = any(y > y_threshold for y in Y_values)\n",
    "\n",
    "    if not x_exceeds and not y_exceeds:\n",
    "        filtered_data.append(entry)\n",
    "    else:\n",
    "        removed_entries.append(entry)\n",
    "print(f\"x < 512 and y < 320\")\n",
    "for item in removed_entries:\n",
    "    print(f\"subject: {item.get('subject')} name: {item.get('name')} X: {item.get('X')} Y: {item.get('Y')}\")\n",
    "\n",
    "# Create the list of slides ['Slide1.jpg', 'Slide3.jpg', ..., 'Slide59.jpg']\n",
    "target_slides = [f'Slide{2*i-1}.jpg' for i in range(1, 31)]\n",
    "\n",
    "# Create a dictionary to track which slides each subject has\n",
    "subjects_slides = {subject: set() for subject in range(1, 12)}\n",
    "\n",
    "# Populate the dictionary with available slides for each subject\n",
    "for record in data:\n",
    "    subject = record['subject']\n",
    "    slide_name = record['name']\n",
    "    if subject in subjects_slides:\n",
    "        subjects_slides[subject].add(slide_name)\n",
    "\n",
    "# Now check for each subject which slides are missing\n",
    "for subject in range(1, 12):\n",
    "    available_slides = subjects_slides[subject]\n",
    "    missing_slides = [slide for slide in target_slides if slide not in available_slides]\n",
    "    \n",
    "    if missing_slides:\n",
    "        print(f\"Subject {subject} is missing the following slides: {', '.join(missing_slides)}\")\n",
    "    else:\n",
    "        print(f\"Subject {subject} has all the slides.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slide11.jpg\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def read_csv(file_path):\n",
    "    # Read CSV file with comma delimiter\n",
    "    df = pd.read_csv(file_path, delimiter=',')\n",
    "    # Clean column names to remove leading/trailing whitespaces and standardize them\n",
    "    df.columns = df.columns.str.strip().str.upper()\n",
    "    return df\n",
    "\n",
    "def extract_rows(df, start, end, time_column):\n",
    "    # Convert time column to numeric\n",
    "    df['timestamp'] = pd.to_numeric(df[time_column], errors='coerce')\n",
    "    # Drop rows with NaN values in 'timestamp' due to conversion issues\n",
    "    df.dropna(subset=['timestamp'], inplace=True)\n",
    "    # Extract rows within the given time range\n",
    "    mask = (df['timestamp'] >= start) & (df['timestamp'] <= end)\n",
    "    return df[mask]\n",
    "\n",
    "def is_within_bbox(x, y, bbox):\n",
    "    x_min, y_min, width, height = bbox\n",
    "    x_max = x_min + width\n",
    "    y_max = y_min + height\n",
    "\n",
    "    # Ensure that x_min <= x_max and y_min <= y_max\n",
    "    x_min, x_max = sorted([x_min, x_max])\n",
    "    y_min, y_max = sorted([y_min, y_max])\n",
    "\n",
    "    return x_min <= x <= x_max and y_min <= y <= y_max\n",
    "\n",
    "def generate_json_entry(name, subject, task, condition, bbox, filtered_df, cx_column, cy_column, duration_column, start_time_in_sec, end_time_in_sec, time_range_in_minutes):\n",
    "    # X_real = filtered_df[cx_column].tolist()\n",
    "    # Y_real = filtered_df[cy_column].tolist()\n",
    "    # Hey chatgpt, Here I want if each if any of X or Y is less than zero put is as zero. For example \n",
    "    # if X is -2 put it as 0 and if Y is -3 put it as 0.\n",
    "    X = [(max(0, x * 512)) for x in filtered_df[cx_column].tolist()]\n",
    "    Y = [(max(0, y * 320)) for y in filtered_df[cy_column].tolist()]\n",
    "    T = filtered_df[duration_column].tolist()\n",
    "    length = len(X)\n",
    "    correct = any(is_within_bbox(X[i], Y[i], bbox) for i in range(length))\n",
    "    \n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"subject\": subject,\n",
    "        # \"time_range_in_seconds\": [start_time_in_sec, end_time_in_sec],\n",
    "        # \"time_range_in_minutes\": time_range_in_minutes,\n",
    "        \"task\": task,\n",
    "        \"condition\": condition,\n",
    "        \"bbox\": bbox,\n",
    "        # \"X_real\": X_real,\n",
    "        # \"Y_real\": Y_real, \n",
    "        \"X\": X,\n",
    "        \"Y\": Y,\n",
    "        \"T\": T,\n",
    "        \"length\": length,\n",
    "        \"correct\": int(correct),\n",
    "        \"split\": \"train\"\n",
    "    }\n",
    "\n",
    "def convert_time_to_seconds(time_str):\n",
    "    # Convert time format (HH:MM:SS:MS) to seconds\n",
    "    hours, minutes, seconds, milliseconds = map(int, time_str.split(':'))\n",
    "    return hours * 3600 + minutes * 60 + seconds + milliseconds / 1000\n",
    "\n",
    "def extract_fixations(file_paths, entries, width=512, height=320):\n",
    "    results = []\n",
    "    BPOGX_column = 'BPOGX'\n",
    "    BPOGY_column = 'BPOGY'\n",
    "    time_column = 'FPOGD'\n",
    "\n",
    "    for entry in entries:\n",
    "        subject = entry['subject']\n",
    "        file_path = file_paths.get(subject)\n",
    "        if not file_path:\n",
    "            continue\n",
    "\n",
    "        df = read_csv(file_path)\n",
    "\n",
    "        if BPOGX_column not in df.columns or BPOGY_column not in df.columns or time_column not in df.columns:\n",
    "            raise KeyError(\"Columns 'BPOGX', 'BPOGY', or 'FPOGD' not found in CSV file.\")\n",
    "\n",
    "        name = entry['name']\n",
    "        task = entry['task']\n",
    "        condition = entry['condition']\n",
    "        bbox_indicator = entry['bbox_indicator']\n",
    "        bbox = [181, 10, 149, 33] if bbox_indicator == 1 else [181, 276, 149, 34]\n",
    "        start, end = entry['time_range']\n",
    "\n",
    "        # Convert time range to seconds\n",
    "        start_time = convert_time_to_seconds(start)\n",
    "        end_time = convert_time_to_seconds(end)\n",
    "\n",
    "        filtered_df = extract_rows(df, start_time, end_time, 'FPOGS')\n",
    "        json_entry = generate_json_entry(name, subject, task, condition, bbox, filtered_df, BPOGX_column, BPOGY_column, time_column, start_time, end_time, entry['time_range'])\n",
    "        if not filtered_df.empty: # Subject 3 Slide19 and Slide21 have empty fixations\n",
    "            results.append(json_entry)\n",
    "\n",
    "    # Limit the size of the JSON by rounding values to reduce length\n",
    "    for entry in results:\n",
    "        entry['X'] = [round(x, 2) for x in entry['X']]\n",
    "        entry['Y'] = [round(y, 2) for y in entry['Y']]\n",
    "        entry['T'] = [round(t*1000) for t in entry['T']]\n",
    "    \n",
    "    return results\n",
    "\n",
    "# List of file paths for each subject\n",
    "file_paths = {\n",
    "    1: '/home/ali/Repos/Scanpath_Prediction/files/Raw_Gaze_Data/P1/GAZE/result/User 0_fixations.csv',\n",
    "    2: '/home/ali/Repos/Scanpath_Prediction/files/Raw_Gaze_Data/P2/GAZE/result/User 2_fixations.csv',\n",
    "    3: '/home/ali/Repos/Scanpath_Prediction/files/Raw_Gaze_Data/P3/GAZE/result/User 1_fixations.csv',\n",
    "    4: '/home/ali/Repos/Scanpath_Prediction/files/Raw_Gaze_Data/P4/GAZE/result/User 0_fixations.csv',\n",
    "    5: '/home/ali/Repos/Scanpath_Prediction/files/Raw_Gaze_Data/P5/GAZE/result/User 0_fixations.csv',\n",
    "    6: '/home/ali/Repos/Scanpath_Prediction/files/Raw_Gaze_Data/P6/Raw Data/GAZE/result/User 0_fixations.csv',\n",
    "    8: '/home/ali/Repos/Scanpath_Prediction/files/Raw_Gaze_Data/P8/GAZE/result/User 0_fixations.csv',\n",
    "    9: '/home/ali/Repos/Scanpath_Prediction/files/Raw_Gaze_Data/P9/GAZE/result/User 1_fixations.csv',\n",
    "    10: '/home/ali/Repos/Scanpath_Prediction/files/Raw_Gaze_Data/P10/GAZE/result/User 0_fixations.csv',\n",
    "    11: '/home/ali/Repos/Scanpath_Prediction/files/Raw_Gaze_Data/P11/GAZE/result/User 0_fixations.csv',\n",
    "}\n",
    "\n",
    "# Load list of entries for fixation extraction from JSON file\n",
    "with open('entries_corrected.json', 'r') as entries_file:\n",
    "    entries = json.load(entries_file)\n",
    "\n",
    "# Extract and scale fixation data\n",
    "fixation_data = extract_fixations(file_paths, entries)\n",
    "\n",
    "# Convert result to JSON\n",
    "with open('fixation_data.json', 'w') as json_file:\n",
    "    json.dump(fixation_data, json_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/data/entries_corrected.json'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uses only for entring data to make all of the times to zero.\n",
    "# import json\n",
    "\n",
    "# # Read the JSON file\n",
    "# with open('entries.json', 'r') as file:\n",
    "#     data = json.load(file)\n",
    "\n",
    "# # Modify the time_range for each entry\n",
    "# for entry in data:\n",
    "#     entry['time_range'] = [\"00:00:00:00\", \"00:00:00:00\"]\n",
    "\n",
    "# # Write to the new JSON file with the time_range in a single line\n",
    "# with open('entries_corrected.json', 'w') as file:\n",
    "#     json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated entries saved to entries_corrected.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Function to update bbox_indicator based on the given dictionary\n",
    "def update_bbox_indicator(file_path, update_dict):\n",
    "    # Load the JSON data from file\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Loop through each entry in the JSON data\n",
    "    for entry in data:\n",
    "        name = entry.get(\"name\")\n",
    "        # Check if the name is in the update dictionary\n",
    "        if name in update_dict:\n",
    "            # Update bbox_indicator with the value from the dictionary\n",
    "            entry[\"bbox_indicator\"] = update_dict[name]\n",
    "    \n",
    "    # Write the updated data back to the JSON file\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "    print(f\"Updated entries saved to {file_path}\")\n",
    "\n",
    "# Example usage\n",
    "file_path = 'entries_corrected.json'\n",
    "update_dict = {\n",
    "    \"Slide1.jpg\": 1,\n",
    "    \"Slide3.jpg\": 0,\n",
    "    \"Slide5.jpg\": 1,\n",
    "    \"Slide7.jpg\": 1,\n",
    "    \"Slide9.jpg\": 0,\n",
    "    \"Slide11.jpg\": 1,\n",
    "    \"Slide13.jpg\": 0,\n",
    "    \"Slide15.jpg\": 0,\n",
    "    \"Slide17.jpg\": 0,\n",
    "    \"Slide19.jpg\": 0,\n",
    "    \"Slide21.jpg\": 1,\n",
    "    \"Slide23.jpg\": 0,\n",
    "    \"Slide25.jpg\": 1,\n",
    "    \"Slide27.jpg\": 0,\n",
    "    \"Slide29.jpg\": 1,\n",
    "    \"Slide31.jpg\": 0,\n",
    "    \"Slide33.jpg\": 1,\n",
    "    \"Slide35.jpg\": 0,\n",
    "    \"Slide37.jpg\": 0,\n",
    "    \"Slide39.jpg\": 1,\n",
    "\n",
    "}\n",
    "\n",
    "update_bbox_indicator(file_path, update_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'normal_Slide1.jpg': [181, 10, 149, 33], 'normal_Slide3.jpg': [181, 276, 149, 34], 'normal_Slide5.jpg': [181, 10, 149, 33], 'normal_Slide7.jpg': [181, 10, 149, 33], 'normal_Slide9.jpg': [181, 276, 149, 34], 'normal_Slide11.jpg': [181, 10, 149, 33], 'normal_Slide13.jpg': [181, 276, 149, 34], 'normal_Slide15.jpg': [181, 276, 149, 34], 'normal_Slide17.jpg': [181, 276, 149, 34], 'normal_Slide19.jpg': [181, 276, 149, 34], 'normal_Slide21.jpg': [181, 10, 149, 33], 'normal_Slide23.jpg': [181, 276, 149, 34], 'normal_Slide25.jpg': [181, 10, 149, 33], 'normal_Slide27.jpg': [181, 276, 149, 34], 'normal_Slide29.jpg': [181, 10, 149, 33], 'normal_Slide31.jpg': [181, 276, 149, 34], 'normal_Slide33.jpg': [181, 10, 149, 33], 'normal_Slide35.jpg': [181, 276, 149, 34], 'normal_Slide37.jpg': [181, 276, 149, 34], 'normal_Slide39.jpg': [181, 10, 149, 33]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Load the JSON file\n",
    "with open('fixation_data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a dictionary to store the data in the desired format\n",
    "bbox_annos = {}\n",
    "\n",
    "# Extract the necessary fields and populate the dictionary\n",
    "for entry in data:\n",
    "    name = entry['name']\n",
    "    task = entry['task']\n",
    "    bbox = entry['bbox']\n",
    "    \n",
    "    # Combine name and task to create a unique key for each entry\n",
    "    key = f\"{task}_{name}\"\n",
    "    bbox_annos[key] = bbox\n",
    "\n",
    "# Save the dictionary as a .npy file\n",
    "np.save('bbox_annos.npy', bbox_annos)\n",
    "\n",
    "# To load and verify the .npy file\n",
    "loaded_bbox_annos = np.load('bbox_annos.npy', allow_pickle=True).item()\n",
    "print(loaded_bbox_annos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been filtered, split, and written to the respective files.\n",
      "Total entries after filtering: 198\n",
      "Training entries: 158\n",
      "Validation entries: 40\n"
     ]
    }
   ],
   "source": [
    "# fixation_data.json to train/valid splits json files with 3 random EEG.\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "json_file_path = 'fixation_data.json'\n",
    "\n",
    "# Output file paths\n",
    "train_file_path = 'files/Stroop_DataSet/stroop_human_scanpath_train_split.json'\n",
    "valid_file_path = 'files/Stroop_DataSet/stroop_human_scanpath_valid_split.json'\n",
    "\n",
    "# Check if the input file exists\n",
    "if not os.path.exists(json_file_path):\n",
    "    print(f\"File not found: {json_file_path}\")\n",
    "    exit(1)\n",
    "\n",
    "# Load the JSON data\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "random.seed(42)\n",
    "# Add 'featureA', 'featureB', and 'featureC' to each entry\n",
    "for entry in data:\n",
    "    # length = entry.get('length', 0)\n",
    "    length = 3\n",
    "    # Ensure length is an integer and greater than 0\n",
    "    if isinstance(length, int) and length > 0:\n",
    "        # Generate random values between 0 and 1\n",
    "        entry['eeg_data'] = [random.random() for _ in range(length)]\n",
    "    else:\n",
    "        # Handle entries without valid 'length'\n",
    "        entry['eeg_data'] = []\n",
    "\n",
    "random.seed(42)\n",
    "# Shuffle the filtered data\n",
    "random.shuffle(data)\n",
    "\n",
    "# Calculate the split index for 80/20 split\n",
    "split_index = int(len(data) * 0.8)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data = data[:split_index]\n",
    "valid_data = data[split_index:]\n",
    "\n",
    "# Update the 'split' field in each entry\n",
    "for entry in train_data:\n",
    "    entry['split'] = 'train'\n",
    "\n",
    "for entry in valid_data:\n",
    "    entry['split'] = 'valid'\n",
    "\n",
    "# Write the training data to 'human_scanpath_train_split.json'\n",
    "with open(train_file_path, 'w') as train_file:\n",
    "    json.dump(train_data, train_file, indent=4)\n",
    "\n",
    "# Write the validation data to 'human_scanpath_valid_split.json'\n",
    "with open(valid_file_path, 'w') as valid_file:\n",
    "    json.dump(valid_data, valid_file, indent=4)\n",
    "\n",
    "print(\"Data has been filtered, split, and written to the respective files.\")\n",
    "print(f\"Total entries after filtering: {len(data)}\")\n",
    "print(f\"Training entries: {len(train_data)}\")\n",
    "print(f\"Validation entries: {len(valid_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
