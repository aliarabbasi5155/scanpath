{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fork_000000537944.jpg': [56, 229, 33, 49],\n",
       " 'knife_000000233539.jpg': [372, 193, 114, 92],\n",
       " 'tv_000000192651.jpg': [51, 138, 60, 54],\n",
       " 'laptop_000000335308.jpg': [16, 210, 52, 39],\n",
       " 'sink_000000164725.jpg': [158, 219, 159, 94],\n",
       " 'fork_000000061672.jpg': [383, 101, 51, 141],\n",
       " 'laptop_000000204979.jpg': [48, 98, 130, 101],\n",
       " 'mouse_000000539056.jpg': [386, 206, 41, 44],\n",
       " 'fork_000000323370.jpg': [399, 55, 65, 184],\n",
       " 'tv_000000203160.jpg': [98, 57, 71, 71]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "bbox_annos = np.load(\"./files/Google Drive/bbox_annos.npy\", allow_pickle=True).item()\n",
    "bbox_annos_sample = {key: bbox_annos[key] for key in list(bbox_annos.keys())[:10]}\n",
    "bbox_annos_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "bbox_annos = np.load(\"./files/Google Drive/bbox_annos.npy\", allow_pickle=True).item()\n",
    "bbox_annos_sample = {key: bbox_annos[key] for key in list(bbox_annos.keys())[:10]}\n",
    "bbox_annos_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been split and written to the respective files.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Replace 'data.json' with the path to your original JSON file\n",
    "with open('files/DCBs_JSONs/dataset_test/SAIL_fixations_TP_train.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Shuffle the data to ensure random splitting\n",
    "random.shuffle(data)\n",
    "\n",
    "# Calculate the split index for 80/20 split\n",
    "split_index = int(len(data) * 0.8)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data = data[:split_index]\n",
    "valid_data = data[split_index:]\n",
    "\n",
    "# Write the training data to 'human_scanpath_train_split.json'\n",
    "with open('files/human_scanpath_train_split.json', 'w') as train_file:\n",
    "    json.dump(train_data, train_file, indent=4)\n",
    "\n",
    "# Write the validation data to 'human_scanpath_valid_split.json'\n",
    "with open('files/human_scanpath_valid_split.json', 'w') as valid_file:\n",
    "    json.dump(valid_data, valid_file, indent=4)\n",
    "\n",
    "print(\"Data has been split and written to the respective files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries checked: 267\n",
      "Entries with values exceeding thresholds: 10\n",
      "\n",
      "Entries exceeding thresholds:\n",
      "\n",
      "Entry Index: 60\n",
      "Name: Slide3.jpg\n",
      "Y values exceeding 320: [749.9488]\n",
      "\n",
      "Entry Index: 62\n",
      "Name: Slide7.jpg\n",
      "Y values exceeding 320: [606.6528]\n",
      "\n",
      "Entry Index: 65\n",
      "Name: Slide13.jpg\n",
      "Y values exceeding 320: [934.8512]\n",
      "\n",
      "Entry Index: 72\n",
      "Name: Slide33.jpg\n",
      "Y values exceeding 320: [324.1504]\n",
      "\n",
      "Entry Index: 120\n",
      "Name: Slide29.jpg\n",
      "Y values exceeding 320: [324.4416]\n",
      "\n",
      "Entry Index: 227\n",
      "Name: Slide3.jpg\n",
      "Y values exceeding 320: [344.2976]\n",
      "\n",
      "Entry Index: 230\n",
      "Name: Slide11.jpg\n",
      "Y values exceeding 320: [354.6464]\n",
      "\n",
      "Entry Index: 231\n",
      "Name: Slide13.jpg\n",
      "Y values exceeding 320: [366.288]\n",
      "\n",
      "Entry Index: 232\n",
      "Name: Slide15.jpg\n",
      "Y values exceeding 320: [333.8688]\n",
      "\n",
      "Entry Index: 243\n",
      "Name: Slide13.jpg\n",
      "Y values exceeding 320: [331.4528]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "json_file_path = 'files/DCBs_JSONs/dataset_test/SAIL_fixations_TP_train.json'\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(json_file_path):\n",
    "    print(f\"File not found: {json_file_path}\")\n",
    "    exit(1)\n",
    "\n",
    "# Load the JSON data\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Initialize counters and lists to store entries exceeding thresholds\n",
    "entries_exceeding_threshold = []\n",
    "total_entries = len(data)\n",
    "entries_with_exceeding_values = 0\n",
    "\n",
    "# Thresholds\n",
    "x_threshold = 512\n",
    "y_threshold = 320\n",
    "\n",
    "# Iterate over each entry in the data\n",
    "for idx, entry in enumerate(data):\n",
    "    X_values = entry.get('X', [])\n",
    "    Y_values = entry.get('Y', [])\n",
    "    entry_name = entry.get('name', f'Entry {idx}')\n",
    "\n",
    "    # Flags to check if any value exceeds the thresholds\n",
    "    x_exceeds = any(x > x_threshold for x in X_values)\n",
    "    y_exceeds = any(y > y_threshold for y in Y_values)\n",
    "\n",
    "    # If any value exceeds the thresholds, store the entry information\n",
    "    if x_exceeds or y_exceeds:\n",
    "        entries_with_exceeding_values += 1\n",
    "        entries_exceeding_threshold.append({\n",
    "            'entry_index': idx,\n",
    "            'name': entry_name,\n",
    "            'X_exceeds': [x for x in X_values if x > x_threshold],\n",
    "            'Y_exceeds': [y for y in Y_values if y > y_threshold]\n",
    "        })\n",
    "\n",
    "# Reporting the results\n",
    "print(f\"Total entries checked: {total_entries}\")\n",
    "print(f\"Entries with values exceeding thresholds: {entries_with_exceeding_values}\\n\")\n",
    "\n",
    "if entries_exceeding_threshold:\n",
    "    print(\"Entries exceeding thresholds:\")\n",
    "    for item in entries_exceeding_threshold:\n",
    "        print(f\"\\nEntry Index: {item['entry_index']}\")\n",
    "        print(f\"Name: {item['name']}\")\n",
    "        if item['X_exceeds']:\n",
    "            print(f\"X values exceeding {x_threshold}: {item['X_exceeds']}\")\n",
    "        if item['Y_exceeds']:\n",
    "            print(f\"Y values exceeding {y_threshold}: {item['Y_exceeds']}\")\n",
    "else:\n",
    "    print(\"No entries have values exceeding the specified thresholds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been filtered, split, and written to the respective files.\n",
      "Total entries after filtering: 257 and 10 entries were removed.\n",
      "Training entries: 205\n",
      "Validation entries: 52\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "json_file_path = 'DCBs_JSONs/dataset_test/SAIL_fixations_TA_train.json'\n",
    "\n",
    "# Output file paths\n",
    "train_file_path = 'human_scanpath_train_split.json'\n",
    "valid_file_path = 'human_scanpath_valid_split.json'\n",
    "\n",
    "# Check if the input file exists\n",
    "if not os.path.exists(json_file_path):\n",
    "    print(f\"File not found: {json_file_path}\")\n",
    "    exit(1)\n",
    "\n",
    "# Load the JSON data\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Thresholds\n",
    "x_threshold = 512\n",
    "y_threshold = 320\n",
    "\n",
    "# Filter entries that have no X[i] > 512 and no Y[i] > 320\n",
    "filtered_data = []\n",
    "for idx, entry in enumerate(data):\n",
    "    X_values = entry.get('X', [])\n",
    "    Y_values = entry.get('Y', [])\n",
    "\n",
    "    # Check if any X[i] > 512 or Y[i] > 320\n",
    "    x_exceeds = any(x > x_threshold for x in X_values)\n",
    "    y_exceeds = any(y > y_threshold for y in Y_values)\n",
    "\n",
    "    if not x_exceeds and not y_exceeds:\n",
    "        filtered_data.append(entry)\n",
    "\n",
    "# Shuffle the filtered data\n",
    "random.shuffle(filtered_data)\n",
    "\n",
    "# Calculate the split index for 80/20 split\n",
    "split_index = int(len(filtered_data) * 0.8)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data = filtered_data[:split_index]\n",
    "valid_data = filtered_data[split_index:]\n",
    "\n",
    "# Update the 'split' field in each entry\n",
    "for entry in train_data:\n",
    "    entry['split'] = 'train'\n",
    "\n",
    "for entry in valid_data:\n",
    "    entry['split'] = 'valid'\n",
    "\n",
    "# Write the training data to 'human_scanpath_train_split.json'\n",
    "with open(train_file_path, 'w') as train_file:\n",
    "    json.dump(train_data, train_file, indent=4)\n",
    "\n",
    "# Write the validation data to 'human_scanpath_valid_split.json'\n",
    "with open(valid_file_path, 'w') as valid_file:\n",
    "    json.dump(valid_data, valid_file, indent=4)\n",
    "\n",
    "print(\"Data has been filtered, split, and written to the respective files.\")\n",
    "print(f\"Total entries after filtering: {len(filtered_data)} and {len(data) - len(filtered_data)} entries were removed.\")\n",
    "print(f\"Training entries: {len(train_data)}\")\n",
    "print(f\"Validation entries: {len(valid_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been filtered, split, and written to the respective files.\n",
      "Total entries after filtering: 142 and 56 entries were removed.\n",
      "Training entries: 113\n",
      "Validation entries: 29\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "json_file_path = 'fixation_data.json'\n",
    "\n",
    "# Output file paths\n",
    "train_file_path = 'stroop_human_scanpath_train_split.json'\n",
    "valid_file_path = 'stroop_human_scanpath_valid_split.json'\n",
    "\n",
    "# Check if the input file exists\n",
    "if not os.path.exists(json_file_path):\n",
    "    print(f\"File not found: {json_file_path}\")\n",
    "    exit(1)\n",
    "\n",
    "# Load the JSON data\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Add 'featureA', 'featureB', and 'featureC' to each entry\n",
    "for entry in data:\n",
    "    # length = entry.get('length', 0)\n",
    "    length = 3\n",
    "    # Ensure length is an integer and greater than 0\n",
    "    if isinstance(length, int) and length > 0:\n",
    "        # Generate random values between 0 and 1\n",
    "        entry['eeg_data'] = [random.random() for _ in range(length)]\n",
    "    else:\n",
    "        # Handle entries without valid 'length'\n",
    "        entry['eeg_data'] = []\n",
    "\n",
    "# Thresholds\n",
    "x_threshold = 512\n",
    "y_threshold = 320\n",
    "\n",
    "# Filter entries that have no X[i] > 512 and no Y[i] > 320\n",
    "filtered_data = []\n",
    "for idx, entry in enumerate(data):\n",
    "    X_values = entry.get('X', [])\n",
    "    Y_values = entry.get('Y', [])\n",
    "\n",
    "    # Check if any X[i] > 512 or Y[i] > 320\n",
    "    x_exceeds = any(x > x_threshold for x in X_values)\n",
    "    y_exceeds = any(y > y_threshold for y in Y_values)\n",
    "\n",
    "    if not x_exceeds and not y_exceeds:\n",
    "        filtered_data.append(entry)\n",
    "\n",
    "# Shuffle the filtered data\n",
    "random.shuffle(filtered_data)\n",
    "\n",
    "# Calculate the split index for 80/20 split\n",
    "split_index = int(len(filtered_data) * 0.8)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data = filtered_data[:split_index]\n",
    "valid_data = filtered_data[split_index:]\n",
    "\n",
    "# Update the 'split' field in each entry\n",
    "for entry in train_data:\n",
    "    entry['split'] = 'train'\n",
    "\n",
    "for entry in valid_data:\n",
    "    entry['split'] = 'valid'\n",
    "\n",
    "# Write the training data to 'human_scanpath_train_split.json'\n",
    "with open(train_file_path, 'w') as train_file:\n",
    "    json.dump(train_data, train_file, indent=4)\n",
    "\n",
    "# Write the validation data to 'human_scanpath_valid_split.json'\n",
    "with open(valid_file_path, 'w') as valid_file:\n",
    "    json.dump(valid_data, valid_file, indent=4)\n",
    "\n",
    "print(\"Data has been filtered, split, and written to the respective files.\")\n",
    "print(f\"Total entries after filtering: {len(filtered_data)} and {len(data) - len(filtered_data)} entries were removed.\")\n",
    "print(f\"Training entries: {len(train_data)}\")\n",
    "print(f\"Validation entries: {len(valid_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_slides: ['Slide1.jpg', 'Slide3.jpg', 'Slide5.jpg', 'Slide7.jpg', 'Slide9.jpg', 'Slide11.jpg', 'Slide13.jpg', 'Slide15.jpg', 'Slide17.jpg', 'Slide19.jpg', 'Slide21.jpg', 'Slide23.jpg', 'Slide25.jpg', 'Slide27.jpg', 'Slide29.jpg', 'Slide31.jpg', 'Slide33.jpg', 'Slide35.jpg', 'Slide37.jpg']\n",
      "subjects_slides: {1: set(), 2: set(), 3: set(), 4: set(), 5: set(), 6: set(), 7: set(), 8: set(), 9: set(), 10: set(), 11: set()}\n",
      "Subject 1 has all the slides.\n",
      "Subject 2 has all the slides.\n",
      "Subject 3 is missing the following slides: Slide19.jpg, Slide21.jpg\n",
      "Subject 4 has all the slides.\n",
      "Subject 5 has all the slides.\n",
      "Subject 6 has all the slides.\n",
      "Subject 7 is missing the following slides: Slide1.jpg, Slide3.jpg, Slide5.jpg, Slide7.jpg, Slide9.jpg, Slide11.jpg, Slide13.jpg, Slide15.jpg, Slide17.jpg, Slide19.jpg, Slide21.jpg, Slide23.jpg, Slide25.jpg, Slide27.jpg, Slide29.jpg, Slide31.jpg, Slide33.jpg, Slide35.jpg, Slide37.jpg\n",
      "Subject 8 has all the slides.\n",
      "Subject 9 has all the slides.\n",
      "Subject 10 has all the slides.\n",
      "Subject 11 has all the slides.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "json_file_path = 'fixation_data.json'\n",
    "\n",
    "# Output file paths\n",
    "train_file_path = 'stroop_human_scanpath_train_split.json'\n",
    "valid_file_path = 'stroop_human_scanpath_valid_split.json'\n",
    "\n",
    "# Check if the input file exists\n",
    "if not os.path.exists(json_file_path):\n",
    "    print(f\"File not found: {json_file_path}\")\n",
    "    exit(1)\n",
    "\n",
    "# Load the JSON data\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Thresholds\n",
    "x_threshold = 512\n",
    "y_threshold = 320\n",
    "\n",
    "# Filter entries that have no X[i] > 512 and no Y[i] > 320\n",
    "filtered_data = []\n",
    "removed_entries = []\n",
    "\n",
    "\n",
    "# Create the list of slides ['Slide1.jpg', 'Slide3.jpg', ..., 'Slide59.jpg']\n",
    "target_slides = [f'Slide{2*i-1}.jpg' for i in range(1, 20)]\n",
    "print(\"target_slides: \" + str(target_slides))\n",
    "\n",
    "# Create a dictionary to track which slides each subject has\n",
    "subjects_slides = {subject: set() for subject in range(1, 12)}\n",
    "\n",
    "# Populate the dictionary with available slides for each subject\n",
    "for record in data:\n",
    "    subject = record['subject']\n",
    "    slide_name = record['name']\n",
    "    if subject in subjects_slides:\n",
    "        subjects_slides[subject].add(slide_name)\n",
    "\n",
    "# Now check for each subject which slides are missing\n",
    "for subject in range(1, 12):\n",
    "    available_slides = subjects_slides[subject]\n",
    "    missing_slides = [slide for slide in target_slides if slide not in available_slides]\n",
    "    \n",
    "    if missing_slides:\n",
    "        print(f\"Subject {subject} is missing the following slides: {', '.join(missing_slides)}\")\n",
    "    else:\n",
    "        print(f\"Subject {subject} has all the slides.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slide11.jpg\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def read_csv(file_path):\n",
    "    # Read CSV file with comma delimiter\n",
    "    df = pd.read_csv(file_path, delimiter=',')\n",
    "    # Clean column names to remove leading/trailing whitespaces and standardize them\n",
    "    df.columns = df.columns.str.strip().str.upper()\n",
    "    return df\n",
    "\n",
    "def extract_rows(df, start, end, time_column):\n",
    "    # Convert time column to numeric\n",
    "    df['timestamp'] = pd.to_numeric(df[time_column], errors='coerce')\n",
    "    # Drop rows with NaN values in 'timestamp' due to conversion issues\n",
    "    df.dropna(subset=['timestamp'], inplace=True)\n",
    "    # Extract rows within the given time range\n",
    "    mask = (df['timestamp'] >= start) & (df['timestamp'] <= end)\n",
    "    return df[mask]\n",
    "\n",
    "def is_within_bbox(x, y, bbox):\n",
    "    x_min, y_min, width, height = bbox\n",
    "    x_max = x_min + width\n",
    "    y_max = y_min + height\n",
    "\n",
    "    # Ensure that x_min <= x_max and y_min <= y_max\n",
    "    x_min, x_max = sorted([x_min, x_max])\n",
    "    y_min, y_max = sorted([y_min, y_max])\n",
    "\n",
    "    return x_min <= x <= x_max and y_min <= y <= y_max\n",
    "\n",
    "def generate_json_entry(name, subject, task, condition, bbox, filtered_df, cx_column, cy_column, duration_column, start_time_in_sec, end_time_in_sec, time_range_in_minutes):\n",
    "    # X_real = filtered_df[cx_column].tolist()\n",
    "    # Y_real = filtered_df[cy_column].tolist()\n",
    "    # Hey chatgpt, Here I want if each if any of X or Y is less than zero put is as zero. For example \n",
    "    # if X is -2 put it as 0 and if Y is -3 put it as 0.\n",
    "    X = [(max(0, x * 512)) for x in filtered_df[cx_column].tolist()]\n",
    "    Y = [(max(0, y * 320)) for y in filtered_df[cy_column].tolist()]\n",
    "    T = filtered_df[duration_column].tolist()\n",
    "    length = len(X)\n",
    "    correct = any(is_within_bbox(X[i], Y[i], bbox) for i in range(length))\n",
    "    \n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"subject\": subject,\n",
    "        # \"time_range_in_seconds\": [start_time_in_sec, end_time_in_sec],\n",
    "        # \"time_range_in_minutes\": time_range_in_minutes,\n",
    "        \"task\": task,\n",
    "        \"condition\": condition,\n",
    "        \"bbox\": bbox,\n",
    "        # \"X_real\": X_real,\n",
    "        # \"Y_real\": Y_real, \n",
    "        \"X\": X,\n",
    "        \"Y\": Y,\n",
    "        \"T\": T,\n",
    "        \"length\": length,\n",
    "        \"correct\": int(correct),\n",
    "        \"split\": \"train\"\n",
    "    }\n",
    "\n",
    "def convert_time_to_seconds(time_str):\n",
    "    # Convert time format (HH:MM:SS:MS) to seconds\n",
    "    hours, minutes, seconds, milliseconds = map(int, time_str.split(':'))\n",
    "    return hours * 3600 + minutes * 60 + seconds + milliseconds / 1000\n",
    "\n",
    "def extract_fixations(file_paths, entries, width=512, height=320):\n",
    "    results = []\n",
    "    BPOGX_column = 'BPOGX'\n",
    "    BPOGY_column = 'BPOGY'\n",
    "    time_column = 'FPOGD'\n",
    "\n",
    "    for entry in entries:\n",
    "        subject = entry['subject']\n",
    "        file_path = file_paths.get(subject)\n",
    "        if not file_path:\n",
    "            continue\n",
    "\n",
    "        df = read_csv(file_path)\n",
    "\n",
    "        if BPOGX_column not in df.columns or BPOGY_column not in df.columns or time_column not in df.columns:\n",
    "            raise KeyError(\"Columns 'BPOGX', 'BPOGY', or 'FPOGD' not found in CSV file.\")\n",
    "\n",
    "        name = entry['name']\n",
    "        task = entry['task']\n",
    "        condition = entry['condition']\n",
    "        bbox_indicator = entry['bbox_indicator']\n",
    "        bbox = [181, 10, 149, 33] if bbox_indicator == 1 else [181, 276, 149, 34]\n",
    "        start, end = entry['time_range']\n",
    "\n",
    "        # Convert time range to seconds\n",
    "        start_time = convert_time_to_seconds(start)\n",
    "        end_time = convert_time_to_seconds(end)\n",
    "\n",
    "        filtered_df = extract_rows(df, start_time, end_time, 'FPOGS')\n",
    "        json_entry = generate_json_entry(name, subject, task, condition, bbox, filtered_df, BPOGX_column, BPOGY_column, time_column, start_time, end_time, entry['time_range'])\n",
    "        if not filtered_df.empty: # Subject 3 Slide19 and Slide21 have empty fixations\n",
    "            results.append(json_entry)\n",
    "\n",
    "    # Limit the size of the JSON by rounding values to reduce length\n",
    "    for entry in results:\n",
    "        entry['X'] = [round(x, 2) for x in entry['X']]\n",
    "        entry['Y'] = [round(y, 2) for y in entry['Y']]\n",
    "        entry['T'] = [round(t*1000) for t in entry['T']]\n",
    "    \n",
    "    return results\n",
    "\n",
    "# List of file paths for each subject\n",
    "file_paths = {\n",
    "    1: '/home/ali/Repos/Scanpath_Prediction/files/Raw_Gaze_Data/P1/GAZE/result/User 0_fixations.csv',\n",
    "    2: '/home/ali/Repos/Scanpath_Prediction/files/Raw_Gaze_Data/P2/GAZE/result/User 2_fixations.csv',\n",
    "    3: '/home/ali/Repos/Scanpath_Prediction/files/Raw_Gaze_Data/P3/GAZE/result/User 1_fixations.csv',\n",
    "    4: '/home/ali/Repos/Scanpath_Prediction/files/Raw_Gaze_Data/P4/GAZE/result/User 0_fixations.csv',\n",
    "    5: '/home/ali/Repos/Scanpath_Prediction/files/Raw_Gaze_Data/P5/GAZE/result/User 0_fixations.csv',\n",
    "    6: '/home/ali/Repos/Scanpath_Prediction/files/Raw_Gaze_Data/P6/Raw Data/GAZE/result/User 0_fixations.csv',\n",
    "    8: '/home/ali/Repos/Scanpath_Prediction/files/Raw_Gaze_Data/P8/GAZE/result/User 0_fixations.csv',\n",
    "    9: '/home/ali/Repos/Scanpath_Prediction/files/Raw_Gaze_Data/P9/GAZE/result/User 1_fixations.csv',\n",
    "    10: '/home/ali/Repos/Scanpath_Prediction/files/Raw_Gaze_Data/P10/GAZE/result/User 0_fixations.csv',\n",
    "    11: '/home/ali/Repos/Scanpath_Prediction/files/Raw_Gaze_Data/P11/GAZE/result/User 0_fixations.csv',\n",
    "}\n",
    "\n",
    "# Load list of entries for fixation extraction from JSON file\n",
    "with open('entries_corrected.json', 'r') as entries_file:\n",
    "    entries = json.load(entries_file)\n",
    "\n",
    "# Extract and scale fixation data\n",
    "fixation_data = extract_fixations(file_paths, entries)\n",
    "\n",
    "# Convert result to JSON\n",
    "with open('fixation_data.json', 'w') as json_file:\n",
    "    json.dump(fixation_data, json_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/data/entries_corrected.json'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uses only for entring data to make all of the times to zero.\n",
    "# import json\n",
    "\n",
    "# # Read the JSON file\n",
    "# with open('entries.json', 'r') as file:\n",
    "#     data = json.load(file)\n",
    "\n",
    "# # Modify the time_range for each entry\n",
    "# for entry in data:\n",
    "#     entry['time_range'] = [\"00:00:00:00\", \"00:00:00:00\"]\n",
    "\n",
    "# # Write to the new JSON file with the time_range in a single line\n",
    "# with open('entries_corrected.json', 'w') as file:\n",
    "#     json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated entries saved to fixation_data.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Function to update bbox_indicator based on the given dictionary\n",
    "def update_bbox_indicator(file_path, update_dict):\n",
    "    # Load the JSON data from file\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Loop through each entry in the JSON data\n",
    "    for entry in data:\n",
    "        name = entry.get(\"name\")\n",
    "        # Check if the name is in the update dictionary\n",
    "        if name in update_dict:\n",
    "            # Update bbox_indicator with the value from the dictionary\n",
    "            entry[\"task\"] = update_dict[name]\n",
    "    \n",
    "    # Write the updated data back to the JSON file\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "    print(f\"Updated entries saved to {file_path}\")\n",
    "\n",
    "# Example usage\n",
    "file_path = 'fixation_data.json'\n",
    "update_dict = {\n",
    "    \"Slide1.jpg\": \"congruent\",\n",
    "    \"Slide3.jpg\": \"incongruent\",\n",
    "    \"Slide5.jpg\": \"congruent\", \n",
    "    \"Slide7.jpg\": \"congruent\",\n",
    "    \"Slide9.jpg\": \"incongruent\",\n",
    "    \"Slide11.jpg\": \"congruent\",\n",
    "    \"Slide13.jpg\": \"incongruent\",\n",
    "    \"Slide15.jpg\": \"incongruent\",\n",
    "    \"Slide17.jpg\": \"incongruent\",\n",
    "    \"Slide19.jpg\": \"incongruent\",\n",
    "    \"Slide21.jpg\": \"congruent\",\n",
    "    \"Slide23.jpg\": \"incongruent\",\n",
    "    \"Slide25.jpg\": \"congruent\",\n",
    "    \"Slide27.jpg\": \"incongruent\",\n",
    "    \"Slide29.jpg\": \"congruent\",\n",
    "    \"Slide31.jpg\": \"incongruent\",\n",
    "    \"Slide33.jpg\": \"congruent\",\n",
    "    \"Slide35.jpg\": \"incongruent\",\n",
    "    \"Slide37.jpg\": \"incongruent\",\n",
    "    \"Slide39.jpg\": \"congruent\",\n",
    "\n",
    "}\n",
    "\n",
    "update_bbox_indicator(file_path, update_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'congruent_Slide1.jpg': [181, 10, 149, 33], 'incongruent_Slide3.jpg': [181, 276, 149, 34], 'congruent_Slide5.jpg': [181, 10, 149, 33], 'congruent_Slide7.jpg': [181, 10, 149, 33], 'incongruent_Slide9.jpg': [181, 276, 149, 34], 'congruent_Slide11.jpg': [181, 10, 149, 33], 'incongruent_Slide13.jpg': [181, 276, 149, 34], 'incongruent_Slide15.jpg': [181, 276, 149, 34], 'incongruent_Slide17.jpg': [181, 276, 149, 34], 'incongruent_Slide19.jpg': [181, 276, 149, 34], 'congruent_Slide21.jpg': [181, 10, 149, 33], 'incongruent_Slide23.jpg': [181, 276, 149, 34], 'congruent_Slide25.jpg': [181, 10, 149, 33], 'incongruent_Slide27.jpg': [181, 276, 149, 34], 'congruent_Slide29.jpg': [181, 10, 149, 33], 'incongruent_Slide31.jpg': [181, 276, 149, 34], 'congruent_Slide33.jpg': [181, 10, 149, 33], 'incongruent_Slide35.jpg': [181, 276, 149, 34], 'incongruent_Slide37.jpg': [181, 276, 149, 34], 'congruent_Slide39.jpg': [181, 10, 149, 33]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Load the JSON file\n",
    "with open('fixation_data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a dictionary to store the data in the desired format\n",
    "bbox_annos = {}\n",
    "\n",
    "# Extract the necessary fields and populate the dictionary\n",
    "for entry in data:\n",
    "    name = entry['name']\n",
    "    task = entry['task']\n",
    "    bbox = entry['bbox']\n",
    "    \n",
    "    # Combine name and task to create a unique key for each entry\n",
    "    key = f\"{task}_{name}\"\n",
    "    bbox_annos[key] = bbox\n",
    "\n",
    "# Save the dictionary as a .npy file\n",
    "np.save('files/Stroop_DataSet/bbox_annos.npy', bbox_annos)\n",
    "\n",
    "# To load and verify the .npy file\n",
    "loaded_bbox_annos = np.load('files/Stroop_DataSet/bbox_annos.npy', allow_pickle=True).item()\n",
    "print(loaded_bbox_annos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in Excel file: Index(['id', 'F1', 'F2', 'F3', 'labels'], dtype='object')\n",
      "JSON file updated successfully!\n"
     ]
    }
   ],
   "source": [
    "# Set Correct EEGs of the Stroop to the fixation_data.json\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load the JSON data\n",
    "with open(\"fixation_data.json\", \"r\") as json_file:\n",
    "    fixation_data = json.load(json_file)\n",
    "\n",
    "# Load the Excel data\n",
    "excel_data = pd.read_excel(\"files/MostImpEEG_Stroop.xlsx\")\n",
    "\n",
    "# Print column names to inspect them\n",
    "print(\"Column names in Excel file:\", excel_data.columns)\n",
    "\n",
    "# Assuming the columns are not labeled B, C, D, and E, replace them with actual column names\n",
    "# Iterate over each JSON record and update based on Excel data\n",
    "excel_index = 0  # Start from the first row in the Excel file\n",
    "skipped_ids = {48, 49}  # Corresponding IDs for subject 3, slides 19 and 21\n",
    "\n",
    "for record in fixation_data:\n",
    "    subject = record[\"subject\"]\n",
    "    slide_number = int(record[\"name\"].replace(\"Slide\", \"\").replace(\".jpg\", \"\"))\n",
    "    \n",
    "    # Calculate the id based on subject and slide\n",
    "    record_id = (subject - 1) * 20 + (slide_number // 2) + 1\n",
    "    \n",
    "\n",
    "    \n",
    "    # Update 'eeg_data' field from F1, F2, and F3 in the Excel file\n",
    "    record[\"eeg_data\"] = [\n",
    "        excel_data.iloc[excel_index][excel_data.columns[1]],  # F1\n",
    "        excel_data.iloc[excel_index][excel_data.columns[2]],  # F2\n",
    "        excel_data.iloc[excel_index][excel_data.columns[3]],  # F3\n",
    "    ]\n",
    "\n",
    "\n",
    "    # I have disabled this currently. I have set in another place\n",
    "    # Update 'correct' field based on label\n",
    "    # label = excel_data.iloc[excel_index][excel_data.columns[4]]\n",
    "    # record[\"task\"] = \"congruent\" if label == \"C\" else \"incongruent\"\n",
    "    # print(f\"subject:{subject} slide_number:{slide_number} record_id:{record_id}\" + \n",
    "    #       f\"excel_index:{excel_index} eeg:{record[\"eeg_data\"]}\")\n",
    "    \n",
    "    # Skip specific slides for subject 3 (slide 19 and slide 21)\n",
    "    if subject == 3 and slide_number == 17:\n",
    "        excel_index += 3\n",
    "        continue\n",
    "    # Increment Excel index to move to the next row\n",
    "    else:\n",
    "        excel_index += 1\n",
    "\n",
    "# Save the updated JSON data\n",
    "with open(\"fixation_data.json\", \"w\") as json_file:\n",
    "    json.dump(fixation_data, json_file, indent=4)\n",
    "\n",
    "print(\"JSON file updated successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been filtered, split, and written to the respective files.\n",
      "Total entries after filtering: 198\n",
      "Training entries: 158\n",
      "Validation entries: 40\n"
     ]
    }
   ],
   "source": [
    "# fixation_data.json to train/valid splits json files with 3 random EEG.\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "json_file_path = 'fixation_data.json'\n",
    "\n",
    "# Output file paths\n",
    "train_file_path = 'files/Stroop_DataSet/stroop_human_scanpath_train_split.json'\n",
    "valid_file_path = 'files/Stroop_DataSet/stroop_human_scanpath_valid_split.json'\n",
    "\n",
    "# Check if the input file exists\n",
    "if not os.path.exists(json_file_path):\n",
    "    print(f\"File not found: {json_file_path}\")\n",
    "    exit(1)\n",
    "\n",
    "# Load the JSON data\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Random EEGs: Because I added real EEGs, I disabled this random EEGs.\n",
    "# random.seed(42)\n",
    "# # Add 'featureA', 'featureB', and 'featureC' to each entry\n",
    "# for entry in data:\n",
    "#     # length = entry.get('length', 0)\n",
    "#     length = 3\n",
    "#     # Ensure length is an integer and greater than 0\n",
    "#     if isinstance(length, int) and length > 0:\n",
    "#         # Generate random values between 0 and 1\n",
    "#         entry['eeg_data'] = [random.random() for _ in range(length)]\n",
    "#     else:\n",
    "#         # Handle entries without valid 'length'\n",
    "#         entry['eeg_data'] = []\n",
    "\n",
    "random.seed(42)\n",
    "# Shuffle the filtered data\n",
    "random.shuffle(data)\n",
    "\n",
    "# Calculate the split index for 80/20 split\n",
    "split_index = int(len(data) * 0.8)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data = data[:split_index]\n",
    "valid_data = data[split_index:]\n",
    "\n",
    "# Update the 'split' field in each entry\n",
    "for entry in train_data:\n",
    "    entry['split'] = 'train'\n",
    "\n",
    "for entry in valid_data:\n",
    "    entry['split'] = 'valid'\n",
    "\n",
    "# Write the training data to 'human_scanpath_train_split.json'\n",
    "with open(train_file_path, 'w') as train_file:\n",
    "    json.dump(train_data, train_file, indent=4)\n",
    "\n",
    "# Write the validation data to 'human_scanpath_valid_split.json'\n",
    "with open(valid_file_path, 'w') as valid_file:\n",
    "    json.dump(valid_data, valid_file, indent=4)\n",
    "\n",
    "print(\"Data has been filtered, split, and written to the respective files.\")\n",
    "print(f\"Total entries after filtering: {len(data)}\")\n",
    "print(f\"Training entries: {len(train_data)}\")\n",
    "print(f\"Validation entries: {len(valid_data)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
