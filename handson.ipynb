{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fork_000000537944.jpg': [56, 229, 33, 49],\n",
       " 'knife_000000233539.jpg': [372, 193, 114, 92],\n",
       " 'tv_000000192651.jpg': [51, 138, 60, 54],\n",
       " 'laptop_000000335308.jpg': [16, 210, 52, 39],\n",
       " 'sink_000000164725.jpg': [158, 219, 159, 94],\n",
       " 'fork_000000061672.jpg': [383, 101, 51, 141],\n",
       " 'laptop_000000204979.jpg': [48, 98, 130, 101],\n",
       " 'mouse_000000539056.jpg': [386, 206, 41, 44],\n",
       " 'fork_000000323370.jpg': [399, 55, 65, 184],\n",
       " 'tv_000000203160.jpg': [98, 57, 71, 71]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "bbox_annos = np.load(\"./Google Drive/bbox_annos.npy\", allow_pickle=True).item()\n",
    "bbox_annos_sample = {key: bbox_annos[key] for key in list(bbox_annos.keys())[:10]}\n",
    "bbox_annos_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "bbox_annos = np.load(\"./Google Drive/bbox_annos.npy\", allow_pickle=True).item()\n",
    "bbox_annos_sample = {key: bbox_annos[key] for key in list(bbox_annos.keys())[:10]}\n",
    "bbox_annos_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been split and written to the respective files.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Replace 'data.json' with the path to your original JSON file\n",
    "with open('DCBs_JSONs/dataset_test/SAIL_fixations_TP_train.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Shuffle the data to ensure random splitting\n",
    "random.shuffle(data)\n",
    "\n",
    "# Calculate the split index for 80/20 split\n",
    "split_index = int(len(data) * 0.8)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data = data[:split_index]\n",
    "valid_data = data[split_index:]\n",
    "\n",
    "# Write the training data to 'human_scanpath_train_split.json'\n",
    "with open('human_scanpath_train_split.json', 'w') as train_file:\n",
    "    json.dump(train_data, train_file, indent=4)\n",
    "\n",
    "# Write the validation data to 'human_scanpath_valid_split.json'\n",
    "with open('human_scanpath_valid_split.json', 'w') as valid_file:\n",
    "    json.dump(valid_data, valid_file, indent=4)\n",
    "\n",
    "print(\"Data has been split and written to the respective files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries checked: 267\n",
      "Entries with values exceeding thresholds: 10\n",
      "\n",
      "Entries exceeding thresholds:\n",
      "\n",
      "Entry Index: 60\n",
      "Name: Slide3.jpg\n",
      "Y values exceeding 320: [749.9488]\n",
      "\n",
      "Entry Index: 62\n",
      "Name: Slide7.jpg\n",
      "Y values exceeding 320: [606.6528]\n",
      "\n",
      "Entry Index: 65\n",
      "Name: Slide13.jpg\n",
      "Y values exceeding 320: [934.8512]\n",
      "\n",
      "Entry Index: 72\n",
      "Name: Slide33.jpg\n",
      "Y values exceeding 320: [324.1504]\n",
      "\n",
      "Entry Index: 120\n",
      "Name: Slide29.jpg\n",
      "Y values exceeding 320: [324.4416]\n",
      "\n",
      "Entry Index: 227\n",
      "Name: Slide3.jpg\n",
      "Y values exceeding 320: [344.2976]\n",
      "\n",
      "Entry Index: 230\n",
      "Name: Slide11.jpg\n",
      "Y values exceeding 320: [354.6464]\n",
      "\n",
      "Entry Index: 231\n",
      "Name: Slide13.jpg\n",
      "Y values exceeding 320: [366.288]\n",
      "\n",
      "Entry Index: 232\n",
      "Name: Slide15.jpg\n",
      "Y values exceeding 320: [333.8688]\n",
      "\n",
      "Entry Index: 243\n",
      "Name: Slide13.jpg\n",
      "Y values exceeding 320: [331.4528]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "json_file_path = 'DCBs_JSONs/dataset_test/SAIL_fixations_TP_train.json'\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(json_file_path):\n",
    "    print(f\"File not found: {json_file_path}\")\n",
    "    exit(1)\n",
    "\n",
    "# Load the JSON data\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Initialize counters and lists to store entries exceeding thresholds\n",
    "entries_exceeding_threshold = []\n",
    "total_entries = len(data)\n",
    "entries_with_exceeding_values = 0\n",
    "\n",
    "# Thresholds\n",
    "x_threshold = 512\n",
    "y_threshold = 320\n",
    "\n",
    "# Iterate over each entry in the data\n",
    "for idx, entry in enumerate(data):\n",
    "    X_values = entry.get('X', [])\n",
    "    Y_values = entry.get('Y', [])\n",
    "    entry_name = entry.get('name', f'Entry {idx}')\n",
    "\n",
    "    # Flags to check if any value exceeds the thresholds\n",
    "    x_exceeds = any(x > x_threshold for x in X_values)\n",
    "    y_exceeds = any(y > y_threshold for y in Y_values)\n",
    "\n",
    "    # If any value exceeds the thresholds, store the entry information\n",
    "    if x_exceeds or y_exceeds:\n",
    "        entries_with_exceeding_values += 1\n",
    "        entries_exceeding_threshold.append({\n",
    "            'entry_index': idx,\n",
    "            'name': entry_name,\n",
    "            'X_exceeds': [x for x in X_values if x > x_threshold],\n",
    "            'Y_exceeds': [y for y in Y_values if y > y_threshold]\n",
    "        })\n",
    "\n",
    "# Reporting the results\n",
    "print(f\"Total entries checked: {total_entries}\")\n",
    "print(f\"Entries with values exceeding thresholds: {entries_with_exceeding_values}\\n\")\n",
    "\n",
    "if entries_exceeding_threshold:\n",
    "    print(\"Entries exceeding thresholds:\")\n",
    "    for item in entries_exceeding_threshold:\n",
    "        print(f\"\\nEntry Index: {item['entry_index']}\")\n",
    "        print(f\"Name: {item['name']}\")\n",
    "        if item['X_exceeds']:\n",
    "            print(f\"X values exceeding {x_threshold}: {item['X_exceeds']}\")\n",
    "        if item['Y_exceeds']:\n",
    "            print(f\"Y values exceeding {y_threshold}: {item['Y_exceeds']}\")\n",
    "else:\n",
    "    print(\"No entries have values exceeding the specified thresholds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been filtered, split, and written to the respective files.\n",
      "Total entries after filtering: 257 and 10 entries were removed.\n",
      "Training entries: 205\n",
      "Validation entries: 52\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "json_file_path = 'DCBs_JSONs/dataset_test/SAIL_fixations_TA_train.json'\n",
    "\n",
    "# Output file paths\n",
    "train_file_path = 'human_scanpath_train_split.json'\n",
    "valid_file_path = 'human_scanpath_valid_split.json'\n",
    "\n",
    "# Check if the input file exists\n",
    "if not os.path.exists(json_file_path):\n",
    "    print(f\"File not found: {json_file_path}\")\n",
    "    exit(1)\n",
    "\n",
    "# Load the JSON data\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Thresholds\n",
    "x_threshold = 512\n",
    "y_threshold = 320\n",
    "\n",
    "# Filter entries that have no X[i] > 512 and no Y[i] > 320\n",
    "filtered_data = []\n",
    "for idx, entry in enumerate(data):\n",
    "    X_values = entry.get('X', [])\n",
    "    Y_values = entry.get('Y', [])\n",
    "\n",
    "    # Check if any X[i] > 512 or Y[i] > 320\n",
    "    x_exceeds = any(x > x_threshold for x in X_values)\n",
    "    y_exceeds = any(y > y_threshold for y in Y_values)\n",
    "\n",
    "    if not x_exceeds and not y_exceeds:\n",
    "        filtered_data.append(entry)\n",
    "\n",
    "# Shuffle the filtered data\n",
    "random.shuffle(filtered_data)\n",
    "\n",
    "# Calculate the split index for 80/20 split\n",
    "split_index = int(len(filtered_data) * 0.8)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data = filtered_data[:split_index]\n",
    "valid_data = filtered_data[split_index:]\n",
    "\n",
    "# Update the 'split' field in each entry\n",
    "for entry in train_data:\n",
    "    entry['split'] = 'train'\n",
    "\n",
    "for entry in valid_data:\n",
    "    entry['split'] = 'valid'\n",
    "\n",
    "# Write the training data to 'human_scanpath_train_split.json'\n",
    "with open(train_file_path, 'w') as train_file:\n",
    "    json.dump(train_data, train_file, indent=4)\n",
    "\n",
    "# Write the validation data to 'human_scanpath_valid_split.json'\n",
    "with open(valid_file_path, 'w') as valid_file:\n",
    "    json.dump(valid_data, valid_file, indent=4)\n",
    "\n",
    "print(\"Data has been filtered, split, and written to the respective files.\")\n",
    "print(f\"Total entries after filtering: {len(filtered_data)} and {len(data) - len(filtered_data)} entries were removed.\")\n",
    "print(f\"Training entries: {len(train_data)}\")\n",
    "print(f\"Validation entries: {len(valid_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been filtered, split, and written to the respective files.\n",
      "Total entries after filtering: 257 and 10 entries were removed.\n",
      "Training entries: 205\n",
      "Validation entries: 52\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "json_file_path = 'DCBs_JSONs/dataset_test/SAIL_fixations_TP_train.json'\n",
    "\n",
    "# Output file paths\n",
    "train_file_path = 'human_scanpath_train_split.json'\n",
    "valid_file_path = 'human_scanpath_valid_split.json'\n",
    "\n",
    "# Check if the input file exists\n",
    "if not os.path.exists(json_file_path):\n",
    "    print(f\"File not found: {json_file_path}\")\n",
    "    exit(1)\n",
    "\n",
    "# Load the JSON data\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Add 'featureA', 'featureB', and 'featureC' to each entry\n",
    "for entry in data:\n",
    "    # length = entry.get('length', 0)\n",
    "    length = 3\n",
    "    # Ensure length is an integer and greater than 0\n",
    "    if isinstance(length, int) and length > 0:\n",
    "        # Generate random values between 0 and 1\n",
    "        entry['eeg_data'] = [random.random() for _ in range(length)]\n",
    "    else:\n",
    "        # Handle entries without valid 'length'\n",
    "        entry['eeg_data'] = []\n",
    "\n",
    "# Thresholds\n",
    "x_threshold = 512\n",
    "y_threshold = 320\n",
    "\n",
    "# Filter entries that have no X[i] > 512 and no Y[i] > 320\n",
    "filtered_data = []\n",
    "for idx, entry in enumerate(data):\n",
    "    X_values = entry.get('X', [])\n",
    "    Y_values = entry.get('Y', [])\n",
    "\n",
    "    # Check if any X[i] > 512 or Y[i] > 320\n",
    "    x_exceeds = any(x > x_threshold for x in X_values)\n",
    "    y_exceeds = any(y > y_threshold for y in Y_values)\n",
    "\n",
    "    if not x_exceeds and not y_exceeds:\n",
    "        filtered_data.append(entry)\n",
    "\n",
    "# Shuffle the filtered data\n",
    "random.shuffle(filtered_data)\n",
    "\n",
    "# Calculate the split index for 80/20 split\n",
    "split_index = int(len(filtered_data) * 0.8)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data = filtered_data[:split_index]\n",
    "valid_data = filtered_data[split_index:]\n",
    "\n",
    "# Update the 'split' field in each entry\n",
    "for entry in train_data:\n",
    "    entry['split'] = 'train'\n",
    "\n",
    "for entry in valid_data:\n",
    "    entry['split'] = 'valid'\n",
    "\n",
    "# Write the training data to 'human_scanpath_train_split.json'\n",
    "with open(train_file_path, 'w') as train_file:\n",
    "    json.dump(train_data, train_file, indent=4)\n",
    "\n",
    "# Write the validation data to 'human_scanpath_valid_split.json'\n",
    "with open(valid_file_path, 'w') as valid_file:\n",
    "    json.dump(valid_data, valid_file, indent=4)\n",
    "\n",
    "print(\"Data has been filtered, split, and written to the respective files.\")\n",
    "print(f\"Total entries after filtering: {len(filtered_data)} and {len(data) - len(filtered_data)} entries were removed.\")\n",
    "print(f\"Training entries: {len(train_data)}\")\n",
    "print(f\"Validation entries: {len(valid_data)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
