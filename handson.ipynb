{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fork_000000537944.jpg': [56, 229, 33, 49],\n",
       " 'knife_000000233539.jpg': [372, 193, 114, 92],\n",
       " 'tv_000000192651.jpg': [51, 138, 60, 54],\n",
       " 'laptop_000000335308.jpg': [16, 210, 52, 39],\n",
       " 'sink_000000164725.jpg': [158, 219, 159, 94],\n",
       " 'fork_000000061672.jpg': [383, 101, 51, 141],\n",
       " 'laptop_000000204979.jpg': [48, 98, 130, 101],\n",
       " 'mouse_000000539056.jpg': [386, 206, 41, 44],\n",
       " 'fork_000000323370.jpg': [399, 55, 65, 184],\n",
       " 'tv_000000203160.jpg': [98, 57, 71, 71]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "bbox_annos = np.load(\"./files/Google Drive/bbox_annos.npy\", allow_pickle=True).item()\n",
    "bbox_annos_sample = {key: bbox_annos[key] for key in list(bbox_annos.keys())[:10]}\n",
    "bbox_annos_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "bbox_annos = np.load(\"./files/Google Drive/bbox_annos.npy\", allow_pickle=True).item()\n",
    "bbox_annos_sample = {key: bbox_annos[key] for key in list(bbox_annos.keys())[:10]}\n",
    "bbox_annos_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been split and written to the respective files.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Replace 'data.json' with the path to your original JSON file\n",
    "with open('files/DCBs_JSONs/dataset_test/SAIL_fixations_TP_train.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Shuffle the data to ensure random splitting\n",
    "random.shuffle(data)\n",
    "\n",
    "# Calculate the split index for 80/20 split\n",
    "split_index = int(len(data) * 0.8)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data = data[:split_index]\n",
    "valid_data = data[split_index:]\n",
    "\n",
    "# Write the training data to 'human_scanpath_train_split.json'\n",
    "with open('files/human_scanpath_train_split.json', 'w') as train_file:\n",
    "    json.dump(train_data, train_file, indent=4)\n",
    "\n",
    "# Write the validation data to 'human_scanpath_valid_split.json'\n",
    "with open('files/human_scanpath_valid_split.json', 'w') as valid_file:\n",
    "    json.dump(valid_data, valid_file, indent=4)\n",
    "\n",
    "print(\"Data has been split and written to the respective files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries checked: 267\n",
      "Entries with values exceeding thresholds: 10\n",
      "\n",
      "Entries exceeding thresholds:\n",
      "\n",
      "Entry Index: 60\n",
      "Name: Slide3.jpg\n",
      "Y values exceeding 320: [749.9488]\n",
      "\n",
      "Entry Index: 62\n",
      "Name: Slide7.jpg\n",
      "Y values exceeding 320: [606.6528]\n",
      "\n",
      "Entry Index: 65\n",
      "Name: Slide13.jpg\n",
      "Y values exceeding 320: [934.8512]\n",
      "\n",
      "Entry Index: 72\n",
      "Name: Slide33.jpg\n",
      "Y values exceeding 320: [324.1504]\n",
      "\n",
      "Entry Index: 120\n",
      "Name: Slide29.jpg\n",
      "Y values exceeding 320: [324.4416]\n",
      "\n",
      "Entry Index: 227\n",
      "Name: Slide3.jpg\n",
      "Y values exceeding 320: [344.2976]\n",
      "\n",
      "Entry Index: 230\n",
      "Name: Slide11.jpg\n",
      "Y values exceeding 320: [354.6464]\n",
      "\n",
      "Entry Index: 231\n",
      "Name: Slide13.jpg\n",
      "Y values exceeding 320: [366.288]\n",
      "\n",
      "Entry Index: 232\n",
      "Name: Slide15.jpg\n",
      "Y values exceeding 320: [333.8688]\n",
      "\n",
      "Entry Index: 243\n",
      "Name: Slide13.jpg\n",
      "Y values exceeding 320: [331.4528]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "json_file_path = 'files/DCBs_JSONs/dataset_test/SAIL_fixations_TP_train.json'\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(json_file_path):\n",
    "    print(f\"File not found: {json_file_path}\")\n",
    "    exit(1)\n",
    "\n",
    "# Load the JSON data\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Initialize counters and lists to store entries exceeding thresholds\n",
    "entries_exceeding_threshold = []\n",
    "total_entries = len(data)\n",
    "entries_with_exceeding_values = 0\n",
    "\n",
    "# Thresholds\n",
    "x_threshold = 512\n",
    "y_threshold = 320\n",
    "\n",
    "# Iterate over each entry in the data\n",
    "for idx, entry in enumerate(data):\n",
    "    X_values = entry.get('X', [])\n",
    "    Y_values = entry.get('Y', [])\n",
    "    entry_name = entry.get('name', f'Entry {idx}')\n",
    "\n",
    "    # Flags to check if any value exceeds the thresholds\n",
    "    x_exceeds = any(x > x_threshold for x in X_values)\n",
    "    y_exceeds = any(y > y_threshold for y in Y_values)\n",
    "\n",
    "    # If any value exceeds the thresholds, store the entry information\n",
    "    if x_exceeds or y_exceeds:\n",
    "        entries_with_exceeding_values += 1\n",
    "        entries_exceeding_threshold.append({\n",
    "            'entry_index': idx,\n",
    "            'name': entry_name,\n",
    "            'X_exceeds': [x for x in X_values if x > x_threshold],\n",
    "            'Y_exceeds': [y for y in Y_values if y > y_threshold]\n",
    "        })\n",
    "\n",
    "# Reporting the results\n",
    "print(f\"Total entries checked: {total_entries}\")\n",
    "print(f\"Entries with values exceeding thresholds: {entries_with_exceeding_values}\\n\")\n",
    "\n",
    "if entries_exceeding_threshold:\n",
    "    print(\"Entries exceeding thresholds:\")\n",
    "    for item in entries_exceeding_threshold:\n",
    "        print(f\"\\nEntry Index: {item['entry_index']}\")\n",
    "        print(f\"Name: {item['name']}\")\n",
    "        if item['X_exceeds']:\n",
    "            print(f\"X values exceeding {x_threshold}: {item['X_exceeds']}\")\n",
    "        if item['Y_exceeds']:\n",
    "            print(f\"Y values exceeding {y_threshold}: {item['Y_exceeds']}\")\n",
    "else:\n",
    "    print(\"No entries have values exceeding the specified thresholds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been filtered, split, and written to the respective files.\n",
      "Total entries after filtering: 257 and 10 entries were removed.\n",
      "Training entries: 205\n",
      "Validation entries: 52\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "json_file_path = 'DCBs_JSONs/dataset_test/SAIL_fixations_TA_train.json'\n",
    "\n",
    "# Output file paths\n",
    "train_file_path = 'human_scanpath_train_split.json'\n",
    "valid_file_path = 'human_scanpath_valid_split.json'\n",
    "\n",
    "# Check if the input file exists\n",
    "if not os.path.exists(json_file_path):\n",
    "    print(f\"File not found: {json_file_path}\")\n",
    "    exit(1)\n",
    "\n",
    "# Load the JSON data\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Thresholds\n",
    "x_threshold = 512\n",
    "y_threshold = 320\n",
    "\n",
    "# Filter entries that have no X[i] > 512 and no Y[i] > 320\n",
    "filtered_data = []\n",
    "for idx, entry in enumerate(data):\n",
    "    X_values = entry.get('X', [])\n",
    "    Y_values = entry.get('Y', [])\n",
    "\n",
    "    # Check if any X[i] > 512 or Y[i] > 320\n",
    "    x_exceeds = any(x > x_threshold for x in X_values)\n",
    "    y_exceeds = any(y > y_threshold for y in Y_values)\n",
    "\n",
    "    if not x_exceeds and not y_exceeds:\n",
    "        filtered_data.append(entry)\n",
    "\n",
    "# Shuffle the filtered data\n",
    "random.shuffle(filtered_data)\n",
    "\n",
    "# Calculate the split index for 80/20 split\n",
    "split_index = int(len(filtered_data) * 0.8)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data = filtered_data[:split_index]\n",
    "valid_data = filtered_data[split_index:]\n",
    "\n",
    "# Update the 'split' field in each entry\n",
    "for entry in train_data:\n",
    "    entry['split'] = 'train'\n",
    "\n",
    "for entry in valid_data:\n",
    "    entry['split'] = 'valid'\n",
    "\n",
    "# Write the training data to 'human_scanpath_train_split.json'\n",
    "with open(train_file_path, 'w') as train_file:\n",
    "    json.dump(train_data, train_file, indent=4)\n",
    "\n",
    "# Write the validation data to 'human_scanpath_valid_split.json'\n",
    "with open(valid_file_path, 'w') as valid_file:\n",
    "    json.dump(valid_data, valid_file, indent=4)\n",
    "\n",
    "print(\"Data has been filtered, split, and written to the respective files.\")\n",
    "print(f\"Total entries after filtering: {len(filtered_data)} and {len(data) - len(filtered_data)} entries were removed.\")\n",
    "print(f\"Training entries: {len(train_data)}\")\n",
    "print(f\"Validation entries: {len(valid_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been filtered, split, and written to the respective files.\n",
      "Total entries after filtering: 257 and 10 entries were removed.\n",
      "Training entries: 205\n",
      "Validation entries: 52\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "json_file_path = 'DCBs_JSONs/dataset_test/SAIL_fixations_TP_train.json'\n",
    "\n",
    "# Output file paths\n",
    "train_file_path = 'human_scanpath_train_split.json'\n",
    "valid_file_path = 'human_scanpath_valid_split.json'\n",
    "\n",
    "# Check if the input file exists\n",
    "if not os.path.exists(json_file_path):\n",
    "    print(f\"File not found: {json_file_path}\")\n",
    "    exit(1)\n",
    "\n",
    "# Load the JSON data\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Add 'featureA', 'featureB', and 'featureC' to each entry\n",
    "for entry in data:\n",
    "    # length = entry.get('length', 0)\n",
    "    length = 3\n",
    "    # Ensure length is an integer and greater than 0\n",
    "    if isinstance(length, int) and length > 0:\n",
    "        # Generate random values between 0 and 1\n",
    "        entry['eeg_data'] = [random.random() for _ in range(length)]\n",
    "    else:\n",
    "        # Handle entries without valid 'length'\n",
    "        entry['eeg_data'] = []\n",
    "\n",
    "# Thresholds\n",
    "x_threshold = 512\n",
    "y_threshold = 320\n",
    "\n",
    "# Filter entries that have no X[i] > 512 and no Y[i] > 320\n",
    "filtered_data = []\n",
    "for idx, entry in enumerate(data):\n",
    "    X_values = entry.get('X', [])\n",
    "    Y_values = entry.get('Y', [])\n",
    "\n",
    "    # Check if any X[i] > 512 or Y[i] > 320\n",
    "    x_exceeds = any(x > x_threshold for x in X_values)\n",
    "    y_exceeds = any(y > y_threshold for y in Y_values)\n",
    "\n",
    "    if not x_exceeds and not y_exceeds:\n",
    "        filtered_data.append(entry)\n",
    "\n",
    "# Shuffle the filtered data\n",
    "random.shuffle(filtered_data)\n",
    "\n",
    "# Calculate the split index for 80/20 split\n",
    "split_index = int(len(filtered_data) * 0.8)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data = filtered_data[:split_index]\n",
    "valid_data = filtered_data[split_index:]\n",
    "\n",
    "# Update the 'split' field in each entry\n",
    "for entry in train_data:\n",
    "    entry['split'] = 'train'\n",
    "\n",
    "for entry in valid_data:\n",
    "    entry['split'] = 'valid'\n",
    "\n",
    "# Write the training data to 'human_scanpath_train_split.json'\n",
    "with open(train_file_path, 'w') as train_file:\n",
    "    json.dump(train_data, train_file, indent=4)\n",
    "\n",
    "# Write the validation data to 'human_scanpath_valid_split.json'\n",
    "with open(valid_file_path, 'w') as valid_file:\n",
    "    json.dump(valid_data, valid_file, indent=4)\n",
    "\n",
    "print(\"Data has been filtered, split, and written to the respective files.\")\n",
    "print(f\"Total entries after filtering: {len(filtered_data)} and {len(data) - len(filtered_data)} entries were removed.\")\n",
    "print(f\"Training entries: {len(train_data)}\")\n",
    "print(f\"Validation entries: {len(valid_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x < 512 and y < 320\n",
      "subject: 3 name: Slide3.jpg X: [247.82848, 50.54368, 262.19008] Y: [135.168, 749.9488, 150.08]\n",
      "subject: 3 name: Slide7.jpg X: [230.3744, 257.67424] Y: [606.6528, 287.2288]\n",
      "subject: 3 name: Slide13.jpg X: [94.65344, 252.1344] Y: [934.8512, 115.408]\n",
      "subject: 3 name: Slide33.jpg X: [254.24384, 248.64256, 267.75552, 267.63776] Y: [153.8432, 156.6176, 303.9232, 324.1504]\n",
      "subject: 5 name: Slide29.jpg X: [214.71232, 200.20736, 272.77824, 257.3824, 265.88672] Y: [140.4384, 150.272, 126.1632, 324.4416, 301.3056]\n",
      "subject: 10 name: Slide3.jpg X: [228.70016, 216.31488] Y: [235.056, 344.2976]\n",
      "subject: 10 name: Slide11.jpg X: [257.4848, 244.52608] Y: [354.6464, 302.4384]\n",
      "subject: 10 name: Slide13.jpg X: [233.07264, 249.58464] Y: [366.288, 310.4704]\n",
      "subject: 10 name: Slide15.jpg X: [246.10816, 258.18112] Y: [333.8688, 279.136]\n",
      "subject: 11 name: Slide13.jpg X: [267.56608, 249.30816, 256.31232] Y: [137.3824, 331.4528, 289.024]\n",
      "Subject 1 is missing the following slides: Slide39.jpg\n",
      "Subject 2 has all the slides.\n",
      "Subject 3 is missing the following slides: Slide19.jpg, Slide29.jpg, Slide31.jpg, Slide37.jpg, Slide39.jpg, Slide41.jpg, Slide43.jpg, Slide45.jpg, Slide47.jpg, Slide49.jpg, Slide51.jpg, Slide53.jpg, Slide55.jpg\n",
      "Subject 4 has all the slides.\n",
      "Subject 5 has all the slides.\n",
      "Subject 6 has all the slides.\n",
      "Subject 7 is missing the following slides: Slide1.jpg, Slide3.jpg, Slide5.jpg, Slide7.jpg, Slide9.jpg, Slide11.jpg, Slide13.jpg, Slide15.jpg, Slide17.jpg, Slide19.jpg, Slide21.jpg, Slide23.jpg, Slide25.jpg, Slide27.jpg, Slide29.jpg, Slide31.jpg, Slide33.jpg, Slide35.jpg, Slide37.jpg, Slide39.jpg, Slide41.jpg, Slide43.jpg, Slide45.jpg, Slide47.jpg, Slide49.jpg, Slide51.jpg, Slide53.jpg, Slide55.jpg, Slide57.jpg, Slide59.jpg\n",
      "Subject 8 has all the slides.\n",
      "Subject 9 has all the slides.\n",
      "Subject 10 is missing the following slides: Slide7.jpg, Slide21.jpg, Slide25.jpg, Slide27.jpg, Slide29.jpg, Slide31.jpg, Slide33.jpg, Slide35.jpg, Slide37.jpg, Slide41.jpg, Slide43.jpg, Slide45.jpg, Slide47.jpg, Slide49.jpg, Slide51.jpg, Slide53.jpg, Slide55.jpg, Slide57.jpg, Slide59.jpg\n",
      "Subject 11 has all the slides.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "json_file_path = 'DCBs_JSONs/dataset_test/SAIL_fixations_TP_train.json'\n",
    "\n",
    "# Output file paths\n",
    "train_file_path = 'human_scanpath_train_split.json'\n",
    "valid_file_path = 'human_scanpath_valid_split.json'\n",
    "\n",
    "# Check if the input file exists\n",
    "if not os.path.exists(json_file_path):\n",
    "    print(f\"File not found: {json_file_path}\")\n",
    "    exit(1)\n",
    "\n",
    "# Load the JSON data\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Thresholds\n",
    "x_threshold = 512\n",
    "y_threshold = 320\n",
    "\n",
    "# Filter entries that have no X[i] > 512 and no Y[i] > 320\n",
    "filtered_data = []\n",
    "removed_entries = []\n",
    "for idx, entry in enumerate(data):\n",
    "    X_values = entry.get('X', [])\n",
    "    Y_values = entry.get('Y', [])\n",
    "\n",
    "    # Check if any X[i] > 512 or Y[i] > 320\n",
    "    x_exceeds = any(x > x_threshold for x in X_values)\n",
    "    y_exceeds = any(y > y_threshold for y in Y_values)\n",
    "\n",
    "    if not x_exceeds and not y_exceeds:\n",
    "        filtered_data.append(entry)\n",
    "    else:\n",
    "        removed_entries.append(entry)\n",
    "print(f\"x < 512 and y < 320\")\n",
    "for item in removed_entries:\n",
    "    print(f\"subject: {item.get('subject')} name: {item.get('name')} X: {item.get('X')} Y: {item.get('Y')}\")\n",
    "\n",
    "# Create the list of slides ['Slide1.jpg', 'Slide3.jpg', ..., 'Slide59.jpg']\n",
    "target_slides = [f'Slide{2*i-1}.jpg' for i in range(1, 31)]\n",
    "\n",
    "# Create a dictionary to track which slides each subject has\n",
    "subjects_slides = {subject: set() for subject in range(1, 12)}\n",
    "\n",
    "# Populate the dictionary with available slides for each subject\n",
    "for record in data:\n",
    "    subject = record['subject']\n",
    "    slide_name = record['name']\n",
    "    if subject in subjects_slides:\n",
    "        subjects_slides[subject].add(slide_name)\n",
    "\n",
    "# Now check for each subject which slides are missing\n",
    "for subject in range(1, 12):\n",
    "    available_slides = subjects_slides[subject]\n",
    "    missing_slides = [slide for slide in target_slides if slide not in available_slides]\n",
    "    \n",
    "    if missing_slides:\n",
    "        print(f\"Subject {subject} is missing the following slides: {', '.join(missing_slides)}\")\n",
    "    else:\n",
    "        print(f\"Subject {subject} has all the slides.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slide11.jpg\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def read_csv(file_path):\n",
    "    # Read CSV file with comma delimiter\n",
    "    df = pd.read_csv(file_path, delimiter=',')\n",
    "    # Clean column names to remove leading/trailing whitespaces and standardize them\n",
    "    df.columns = df.columns.str.strip().str.upper()\n",
    "    return df\n",
    "\n",
    "def extract_rows(df, start, end, time_column):\n",
    "    # Convert time column to numeric\n",
    "    df['timestamp'] = pd.to_numeric(df[time_column], errors='coerce')\n",
    "    # Drop rows with NaN values in 'timestamp' due to conversion issues\n",
    "    df.dropna(subset=['timestamp'], inplace=True)\n",
    "    # Extract rows within the given time range\n",
    "    mask = (df['timestamp'] >= start) & (df['timestamp'] <= end)\n",
    "    return df[mask]\n",
    "\n",
    "def is_within_bbox(x, y, bbox):\n",
    "    x_min, y_min, width, height = bbox\n",
    "    x_max, y_max = x_min + width, y_min + height\n",
    "    return x_min <= x <= x_max and y_min <= y <= y_max\n",
    "\n",
    "def generate_json_entry(name, subject, task, condition, bbox, filtered_df, cx_column, cy_column, duration_column, start_time_in_sec, end_time_in_sec, time_range_in_minutes):\n",
    "    # X_real = filtered_df[cx_column].tolist()\n",
    "    # Y_real = filtered_df[cy_column].tolist()\n",
    "    # Hey chatgpt, Here I want if each if any of X or Y is less than zero put is as zero. For example \n",
    "    # if X is -2 put it as 0 and if Y is -3 put it as 0.\n",
    "    X = [(max(0, x * 512)) for x in filtered_df[cx_column].tolist()]\n",
    "    Y = [(max(0, y * 320)) for y in filtered_df[cy_column].tolist()]\n",
    "    T = filtered_df[duration_column].tolist()\n",
    "    length = len(X)\n",
    "    correct = any(is_within_bbox(X[i], Y[i], bbox) for i in range(length))\n",
    "    \n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"subject\": subject,\n",
    "        # \"time_range_in_seconds\": [start_time_in_sec, end_time_in_sec],\n",
    "        # \"time_range_in_minutes\": time_range_in_minutes,\n",
    "        \"task\": task,\n",
    "        \"condition\": condition,\n",
    "        \"bbox\": bbox,\n",
    "        # \"X_real\": X_real,\n",
    "        # \"Y_real\": Y_real, \n",
    "        \"X\": X,\n",
    "        \"Y\": Y,\n",
    "        \"T\": T,\n",
    "        \"length\": length,\n",
    "        \"correct\": int(correct),\n",
    "        \"split\": \"train\"\n",
    "    }\n",
    "\n",
    "def convert_time_to_seconds(time_str):\n",
    "    # Convert time format (HH:MM:SS:MS) to seconds\n",
    "    hours, minutes, seconds, milliseconds = map(int, time_str.split(':'))\n",
    "    return hours * 3600 + minutes * 60 + seconds + milliseconds / 1000\n",
    "\n",
    "def extract_fixations(file_paths, entries, width=512, height=320):\n",
    "    results = []\n",
    "    BPOGX_column = 'BPOGX'\n",
    "    BPOGY_column = 'BPOGY'\n",
    "    time_column = 'FPOGD'\n",
    "\n",
    "    for entry in entries:\n",
    "        subject = entry['subject']\n",
    "        file_path = file_paths.get(subject)\n",
    "        if not file_path:\n",
    "            continue\n",
    "\n",
    "        df = read_csv(file_path)\n",
    "\n",
    "        if BPOGX_column not in df.columns or BPOGY_column not in df.columns or time_column not in df.columns:\n",
    "            raise KeyError(\"Columns 'BPOGX', 'BPOGY', or 'FPOGD' not found in CSV file.\")\n",
    "\n",
    "        name = entry['name']\n",
    "        task = entry['task']\n",
    "        condition = entry['condition']\n",
    "        bbox_indicator = entry['bbox_indicator']\n",
    "        bbox = [181, 43, 149, 33] if bbox_indicator == 1 else [181, 310, 149, 33]\n",
    "        start, end = entry['time_range']\n",
    "\n",
    "        # Convert time range to seconds\n",
    "        start_time = convert_time_to_seconds(start)\n",
    "        end_time = convert_time_to_seconds(end)\n",
    "\n",
    "        filtered_df = extract_rows(df, start_time, end_time, 'FPOGS')\n",
    "        json_entry = generate_json_entry(name, subject, task, condition, bbox, filtered_df, BPOGX_column, BPOGY_column, time_column, start_time, end_time, entry['time_range'])\n",
    "        if not filtered_df.empty: # Subject 3 Slide19 and Slide21 have empty fixations\n",
    "            results.append(json_entry)\n",
    "\n",
    "    # Limit the size of the JSON by rounding values to reduce length\n",
    "    for entry in results:\n",
    "        entry['X'] = [round(x, 2) for x in entry['X']]\n",
    "        entry['Y'] = [round(y, 2) for y in entry['Y']]\n",
    "        entry['T'] = [round(t*1000) for t in entry['T']]\n",
    "    \n",
    "    return results\n",
    "\n",
    "# List of file paths for each subject\n",
    "file_paths = {\n",
    "    1: '/home/ali/Repos/Scanpath_Prediction/files/Raw_Gaze_Data/P1/GAZE/result/User 0_fixations.csv',\n",
    "    2: '/home/ali/Repos/Scanpath_Prediction/files/Raw_Gaze_Data/P2/GAZE/result/User 2_fixations.csv',\n",
    "    3: '/home/ali/Repos/Scanpath_Prediction/files/Raw_Gaze_Data/P3/GAZE/result/User 1_fixations.csv',\n",
    "    4: '/home/ali/Repos/Scanpath_Prediction/files/Raw_Gaze_Data/P4/GAZE/result/User 0_fixations.csv',\n",
    "    5: '/home/ali/Repos/Scanpath_Prediction/files/Raw_Gaze_Data/P5/GAZE/result/User 0_fixations.csv',\n",
    "    6: '/home/ali/Repos/Scanpath_Prediction/files/Raw_Gaze_Data/P6/Raw Data/GAZE/result/User 0_fixations.csv',\n",
    "    8: '/home/ali/Repos/Scanpath_Prediction/files/Raw_Gaze_Data/P8/GAZE/result/User 0_fixations.csv',\n",
    "    9: '/home/ali/Repos/Scanpath_Prediction/files/Raw_Gaze_Data/P9/GAZE/result/User 1_fixations.csv',\n",
    "    10: '/home/ali/Repos/Scanpath_Prediction/files/Raw_Gaze_Data/P10/GAZE/result/User 0_fixations.csv',\n",
    "    11: '/home/ali/Repos/Scanpath_Prediction/files/Raw_Gaze_Data/P11/GAZE/result/User 0_fixations.csv',\n",
    "}\n",
    "\n",
    "# Load list of entries for fixation extraction from JSON file\n",
    "with open('entries_corrected.json', 'r') as entries_file:\n",
    "    entries = json.load(entries_file)\n",
    "\n",
    "# Extract and scale fixation data\n",
    "fixation_data = extract_fixations(file_paths, entries)\n",
    "\n",
    "# Convert result to JSON\n",
    "with open('fixation_data.json', 'w') as json_file:\n",
    "    json.dump(fixation_data, json_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/data/entries_corrected.json'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uses only for entring data to make all of the times to zero.\n",
    "# import json\n",
    "\n",
    "# # Read the JSON file\n",
    "# with open('entries.json', 'r') as file:\n",
    "#     data = json.load(file)\n",
    "\n",
    "# # Modify the time_range for each entry\n",
    "# for entry in data:\n",
    "#     entry['time_range'] = [\"00:00:00:00\", \"00:00:00:00\"]\n",
    "\n",
    "# # Write to the new JSON file with the time_range in a single line\n",
    "# with open('entries_corrected.json', 'w') as file:\n",
    "#     json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated entries saved to entries_corrected.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Function to update bbox_indicator based on the given dictionary\n",
    "def update_bbox_indicator(file_path, update_dict):\n",
    "    # Load the JSON data from file\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Loop through each entry in the JSON data\n",
    "    for entry in data:\n",
    "        name = entry.get(\"name\")\n",
    "        # Check if the name is in the update dictionary\n",
    "        if name in update_dict:\n",
    "            # Update bbox_indicator with the value from the dictionary\n",
    "            entry[\"bbox_indicator\"] = update_dict[name]\n",
    "    \n",
    "    # Write the updated data back to the JSON file\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "    print(f\"Updated entries saved to {file_path}\")\n",
    "\n",
    "# Example usage\n",
    "file_path = 'entries_corrected.json'\n",
    "update_dict = {\n",
    "    \"Slide1.jpg\": 1,\n",
    "    \"Slide3.jpg\": 0,\n",
    "    \"Slide5.jpg\": 1,\n",
    "    \"Slide7.jpg\": 1,\n",
    "    \"Slide9.jpg\": 0,\n",
    "    \"Slide11.jpg\": 1,\n",
    "    \"Slide13.jpg\": 0,\n",
    "    \"Slide15.jpg\": 0,\n",
    "    \"Slide17.jpg\": 0,\n",
    "    \"Slide19.jpg\": 0,\n",
    "    \"Slide21.jpg\": 1,\n",
    "    \"Slide23.jpg\": 0,\n",
    "    \"Slide25.jpg\": 1,\n",
    "    \"Slide27.jpg\": 0,\n",
    "    \"Slide29.jpg\": 1,\n",
    "    \"Slide31.jpg\": 0,\n",
    "    \"Slide33.jpg\": 1,\n",
    "    \"Slide35.jpg\": 0,\n",
    "    \"Slide37.jpg\": 0,\n",
    "    \"Slide39.jpg\": 1,\n",
    "\n",
    "}\n",
    "\n",
    "update_bbox_indicator(file_path, update_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
