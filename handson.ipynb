{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fork_000000537944.jpg': [56, 229, 33, 49],\n",
       " 'knife_000000233539.jpg': [372, 193, 114, 92],\n",
       " 'tv_000000192651.jpg': [51, 138, 60, 54],\n",
       " 'laptop_000000335308.jpg': [16, 210, 52, 39],\n",
       " 'sink_000000164725.jpg': [158, 219, 159, 94],\n",
       " 'fork_000000061672.jpg': [383, 101, 51, 141],\n",
       " 'laptop_000000204979.jpg': [48, 98, 130, 101],\n",
       " 'mouse_000000539056.jpg': [386, 206, 41, 44],\n",
       " 'fork_000000323370.jpg': [399, 55, 65, 184],\n",
       " 'tv_000000203160.jpg': [98, 57, 71, 71]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "bbox_annos = np.load(\"./Google Drive/bbox_annos.npy\", allow_pickle=True).item()\n",
    "bbox_annos_sample = {key: bbox_annos[key] for key in list(bbox_annos.keys())[:10]}\n",
    "bbox_annos_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "bbox_annos = np.load(\"./Google Drive/bbox_annos.npy\", allow_pickle=True).item()\n",
    "bbox_annos_sample = {key: bbox_annos[key] for key in list(bbox_annos.keys())[:10]}\n",
    "bbox_annos_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been split and written to the respective files.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Replace 'data.json' with the path to your original JSON file\n",
    "with open('DCBs_JSONs/dataset_test/SAIL_fixations_TP_train.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Shuffle the data to ensure random splitting\n",
    "random.shuffle(data)\n",
    "\n",
    "# Calculate the split index for 80/20 split\n",
    "split_index = int(len(data) * 0.8)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data = data[:split_index]\n",
    "valid_data = data[split_index:]\n",
    "\n",
    "# Write the training data to 'human_scanpath_train_split.json'\n",
    "with open('human_scanpath_train_split.json', 'w') as train_file:\n",
    "    json.dump(train_data, train_file, indent=4)\n",
    "\n",
    "# Write the validation data to 'human_scanpath_valid_split.json'\n",
    "with open('human_scanpath_valid_split.json', 'w') as valid_file:\n",
    "    json.dump(valid_data, valid_file, indent=4)\n",
    "\n",
    "print(\"Data has been split and written to the respective files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries checked: 267\n",
      "Entries with values exceeding thresholds: 10\n",
      "\n",
      "Entries exceeding thresholds:\n",
      "\n",
      "Entry Index: 60\n",
      "Name: Slide3.jpg\n",
      "Y values exceeding 320: [749.9488]\n",
      "\n",
      "Entry Index: 62\n",
      "Name: Slide7.jpg\n",
      "Y values exceeding 320: [606.6528]\n",
      "\n",
      "Entry Index: 65\n",
      "Name: Slide13.jpg\n",
      "Y values exceeding 320: [934.8512]\n",
      "\n",
      "Entry Index: 72\n",
      "Name: Slide33.jpg\n",
      "Y values exceeding 320: [324.1504]\n",
      "\n",
      "Entry Index: 120\n",
      "Name: Slide29.jpg\n",
      "Y values exceeding 320: [324.4416]\n",
      "\n",
      "Entry Index: 227\n",
      "Name: Slide3.jpg\n",
      "Y values exceeding 320: [344.2976]\n",
      "\n",
      "Entry Index: 230\n",
      "Name: Slide11.jpg\n",
      "Y values exceeding 320: [354.6464]\n",
      "\n",
      "Entry Index: 231\n",
      "Name: Slide13.jpg\n",
      "Y values exceeding 320: [366.288]\n",
      "\n",
      "Entry Index: 232\n",
      "Name: Slide15.jpg\n",
      "Y values exceeding 320: [333.8688]\n",
      "\n",
      "Entry Index: 243\n",
      "Name: Slide13.jpg\n",
      "Y values exceeding 320: [331.4528]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "json_file_path = 'DCBs_JSONs/dataset_test/SAIL_fixations_TP_train.json'\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(json_file_path):\n",
    "    print(f\"File not found: {json_file_path}\")\n",
    "    exit(1)\n",
    "\n",
    "# Load the JSON data\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Initialize counters and lists to store entries exceeding thresholds\n",
    "entries_exceeding_threshold = []\n",
    "total_entries = len(data)\n",
    "entries_with_exceeding_values = 0\n",
    "\n",
    "# Thresholds\n",
    "x_threshold = 512\n",
    "y_threshold = 320\n",
    "\n",
    "# Iterate over each entry in the data\n",
    "for idx, entry in enumerate(data):\n",
    "    X_values = entry.get('X', [])\n",
    "    Y_values = entry.get('Y', [])\n",
    "    entry_name = entry.get('name', f'Entry {idx}')\n",
    "\n",
    "    # Flags to check if any value exceeds the thresholds\n",
    "    x_exceeds = any(x > x_threshold for x in X_values)\n",
    "    y_exceeds = any(y > y_threshold for y in Y_values)\n",
    "\n",
    "    # If any value exceeds the thresholds, store the entry information\n",
    "    if x_exceeds or y_exceeds:\n",
    "        entries_with_exceeding_values += 1\n",
    "        entries_exceeding_threshold.append({\n",
    "            'entry_index': idx,\n",
    "            'name': entry_name,\n",
    "            'X_exceeds': [x for x in X_values if x > x_threshold],\n",
    "            'Y_exceeds': [y for y in Y_values if y > y_threshold]\n",
    "        })\n",
    "\n",
    "# Reporting the results\n",
    "print(f\"Total entries checked: {total_entries}\")\n",
    "print(f\"Entries with values exceeding thresholds: {entries_with_exceeding_values}\\n\")\n",
    "\n",
    "if entries_exceeding_threshold:\n",
    "    print(\"Entries exceeding thresholds:\")\n",
    "    for item in entries_exceeding_threshold:\n",
    "        print(f\"\\nEntry Index: {item['entry_index']}\")\n",
    "        print(f\"Name: {item['name']}\")\n",
    "        if item['X_exceeds']:\n",
    "            print(f\"X values exceeding {x_threshold}: {item['X_exceeds']}\")\n",
    "        if item['Y_exceeds']:\n",
    "            print(f\"Y values exceeding {y_threshold}: {item['Y_exceeds']}\")\n",
    "else:\n",
    "    print(\"No entries have values exceeding the specified thresholds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been filtered, split, and written to the respective files.\n",
      "Total entries after filtering: 257 and 10 entries were removed.\n",
      "Training entries: 205\n",
      "Validation entries: 52\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "json_file_path = 'DCBs_JSONs/dataset_test/SAIL_fixations_TA_train.json'\n",
    "\n",
    "# Output file paths\n",
    "train_file_path = 'human_scanpath_train_split.json'\n",
    "valid_file_path = 'human_scanpath_valid_split.json'\n",
    "\n",
    "# Check if the input file exists\n",
    "if not os.path.exists(json_file_path):\n",
    "    print(f\"File not found: {json_file_path}\")\n",
    "    exit(1)\n",
    "\n",
    "# Load the JSON data\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Thresholds\n",
    "x_threshold = 512\n",
    "y_threshold = 320\n",
    "\n",
    "# Filter entries that have no X[i] > 512 and no Y[i] > 320\n",
    "filtered_data = []\n",
    "for idx, entry in enumerate(data):\n",
    "    X_values = entry.get('X', [])\n",
    "    Y_values = entry.get('Y', [])\n",
    "\n",
    "    # Check if any X[i] > 512 or Y[i] > 320\n",
    "    x_exceeds = any(x > x_threshold for x in X_values)\n",
    "    y_exceeds = any(y > y_threshold for y in Y_values)\n",
    "\n",
    "    if not x_exceeds and not y_exceeds:\n",
    "        filtered_data.append(entry)\n",
    "\n",
    "# Shuffle the filtered data\n",
    "random.shuffle(filtered_data)\n",
    "\n",
    "# Calculate the split index for 80/20 split\n",
    "split_index = int(len(filtered_data) * 0.8)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data = filtered_data[:split_index]\n",
    "valid_data = filtered_data[split_index:]\n",
    "\n",
    "# Update the 'split' field in each entry\n",
    "for entry in train_data:\n",
    "    entry['split'] = 'train'\n",
    "\n",
    "for entry in valid_data:\n",
    "    entry['split'] = 'valid'\n",
    "\n",
    "# Write the training data to 'human_scanpath_train_split.json'\n",
    "with open(train_file_path, 'w') as train_file:\n",
    "    json.dump(train_data, train_file, indent=4)\n",
    "\n",
    "# Write the validation data to 'human_scanpath_valid_split.json'\n",
    "with open(valid_file_path, 'w') as valid_file:\n",
    "    json.dump(valid_data, valid_file, indent=4)\n",
    "\n",
    "print(\"Data has been filtered, split, and written to the respective files.\")\n",
    "print(f\"Total entries after filtering: {len(filtered_data)} and {len(data) - len(filtered_data)} entries were removed.\")\n",
    "print(f\"Training entries: {len(train_data)}\")\n",
    "print(f\"Validation entries: {len(valid_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been filtered, split, and written to the respective files.\n",
      "Total entries after filtering: 257 and 10 entries were removed.\n",
      "Training entries: 205\n",
      "Validation entries: 52\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "json_file_path = 'DCBs_JSONs/dataset_test/SAIL_fixations_TP_train.json'\n",
    "\n",
    "# Output file paths\n",
    "train_file_path = 'human_scanpath_train_split.json'\n",
    "valid_file_path = 'human_scanpath_valid_split.json'\n",
    "\n",
    "# Check if the input file exists\n",
    "if not os.path.exists(json_file_path):\n",
    "    print(f\"File not found: {json_file_path}\")\n",
    "    exit(1)\n",
    "\n",
    "# Load the JSON data\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Add 'featureA', 'featureB', and 'featureC' to each entry\n",
    "for entry in data:\n",
    "    # length = entry.get('length', 0)\n",
    "    length = 3\n",
    "    # Ensure length is an integer and greater than 0\n",
    "    if isinstance(length, int) and length > 0:\n",
    "        # Generate random values between 0 and 1\n",
    "        entry['eeg_data'] = [random.random() for _ in range(length)]\n",
    "    else:\n",
    "        # Handle entries without valid 'length'\n",
    "        entry['eeg_data'] = []\n",
    "\n",
    "# Thresholds\n",
    "x_threshold = 512\n",
    "y_threshold = 320\n",
    "\n",
    "# Filter entries that have no X[i] > 512 and no Y[i] > 320\n",
    "filtered_data = []\n",
    "for idx, entry in enumerate(data):\n",
    "    X_values = entry.get('X', [])\n",
    "    Y_values = entry.get('Y', [])\n",
    "\n",
    "    # Check if any X[i] > 512 or Y[i] > 320\n",
    "    x_exceeds = any(x > x_threshold for x in X_values)\n",
    "    y_exceeds = any(y > y_threshold for y in Y_values)\n",
    "\n",
    "    if not x_exceeds and not y_exceeds:\n",
    "        filtered_data.append(entry)\n",
    "\n",
    "# Shuffle the filtered data\n",
    "random.shuffle(filtered_data)\n",
    "\n",
    "# Calculate the split index for 80/20 split\n",
    "split_index = int(len(filtered_data) * 0.8)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data = filtered_data[:split_index]\n",
    "valid_data = filtered_data[split_index:]\n",
    "\n",
    "# Update the 'split' field in each entry\n",
    "for entry in train_data:\n",
    "    entry['split'] = 'train'\n",
    "\n",
    "for entry in valid_data:\n",
    "    entry['split'] = 'valid'\n",
    "\n",
    "# Write the training data to 'human_scanpath_train_split.json'\n",
    "with open(train_file_path, 'w') as train_file:\n",
    "    json.dump(train_data, train_file, indent=4)\n",
    "\n",
    "# Write the validation data to 'human_scanpath_valid_split.json'\n",
    "with open(valid_file_path, 'w') as valid_file:\n",
    "    json.dump(valid_data, valid_file, indent=4)\n",
    "\n",
    "print(\"Data has been filtered, split, and written to the respective files.\")\n",
    "print(f\"Total entries after filtering: {len(filtered_data)} and {len(data) - len(filtered_data)} entries were removed.\")\n",
    "print(f\"Training entries: {len(train_data)}\")\n",
    "print(f\"Validation entries: {len(valid_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x < 512 and y < 320\n",
      "subject: 3 name: Slide3.jpg X: [247.82848, 50.54368, 262.19008] Y: [135.168, 749.9488, 150.08]\n",
      "subject: 3 name: Slide7.jpg X: [230.3744, 257.67424] Y: [606.6528, 287.2288]\n",
      "subject: 3 name: Slide13.jpg X: [94.65344, 252.1344] Y: [934.8512, 115.408]\n",
      "subject: 3 name: Slide33.jpg X: [254.24384, 248.64256, 267.75552, 267.63776] Y: [153.8432, 156.6176, 303.9232, 324.1504]\n",
      "subject: 5 name: Slide29.jpg X: [214.71232, 200.20736, 272.77824, 257.3824, 265.88672] Y: [140.4384, 150.272, 126.1632, 324.4416, 301.3056]\n",
      "subject: 10 name: Slide3.jpg X: [228.70016, 216.31488] Y: [235.056, 344.2976]\n",
      "subject: 10 name: Slide11.jpg X: [257.4848, 244.52608] Y: [354.6464, 302.4384]\n",
      "subject: 10 name: Slide13.jpg X: [233.07264, 249.58464] Y: [366.288, 310.4704]\n",
      "subject: 10 name: Slide15.jpg X: [246.10816, 258.18112] Y: [333.8688, 279.136]\n",
      "subject: 11 name: Slide13.jpg X: [267.56608, 249.30816, 256.31232] Y: [137.3824, 331.4528, 289.024]\n",
      "Subject 1 is missing the following slides: Slide39.jpg\n",
      "Subject 2 has all the slides.\n",
      "Subject 3 is missing the following slides: Slide19.jpg, Slide29.jpg, Slide31.jpg, Slide37.jpg, Slide39.jpg, Slide41.jpg, Slide43.jpg, Slide45.jpg, Slide47.jpg, Slide49.jpg, Slide51.jpg, Slide53.jpg, Slide55.jpg\n",
      "Subject 4 has all the slides.\n",
      "Subject 5 has all the slides.\n",
      "Subject 6 has all the slides.\n",
      "Subject 7 is missing the following slides: Slide1.jpg, Slide3.jpg, Slide5.jpg, Slide7.jpg, Slide9.jpg, Slide11.jpg, Slide13.jpg, Slide15.jpg, Slide17.jpg, Slide19.jpg, Slide21.jpg, Slide23.jpg, Slide25.jpg, Slide27.jpg, Slide29.jpg, Slide31.jpg, Slide33.jpg, Slide35.jpg, Slide37.jpg, Slide39.jpg, Slide41.jpg, Slide43.jpg, Slide45.jpg, Slide47.jpg, Slide49.jpg, Slide51.jpg, Slide53.jpg, Slide55.jpg, Slide57.jpg, Slide59.jpg\n",
      "Subject 8 has all the slides.\n",
      "Subject 9 has all the slides.\n",
      "Subject 10 is missing the following slides: Slide7.jpg, Slide21.jpg, Slide25.jpg, Slide27.jpg, Slide29.jpg, Slide31.jpg, Slide33.jpg, Slide35.jpg, Slide37.jpg, Slide41.jpg, Slide43.jpg, Slide45.jpg, Slide47.jpg, Slide49.jpg, Slide51.jpg, Slide53.jpg, Slide55.jpg, Slide57.jpg, Slide59.jpg\n",
      "Subject 11 has all the slides.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "json_file_path = 'DCBs_JSONs/dataset_test/SAIL_fixations_TP_train.json'\n",
    "\n",
    "# Output file paths\n",
    "train_file_path = 'human_scanpath_train_split.json'\n",
    "valid_file_path = 'human_scanpath_valid_split.json'\n",
    "\n",
    "# Check if the input file exists\n",
    "if not os.path.exists(json_file_path):\n",
    "    print(f\"File not found: {json_file_path}\")\n",
    "    exit(1)\n",
    "\n",
    "# Load the JSON data\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Thresholds\n",
    "x_threshold = 512\n",
    "y_threshold = 320\n",
    "\n",
    "# Filter entries that have no X[i] > 512 and no Y[i] > 320\n",
    "filtered_data = []\n",
    "removed_entries = []\n",
    "for idx, entry in enumerate(data):\n",
    "    X_values = entry.get('X', [])\n",
    "    Y_values = entry.get('Y', [])\n",
    "\n",
    "    # Check if any X[i] > 512 or Y[i] > 320\n",
    "    x_exceeds = any(x > x_threshold for x in X_values)\n",
    "    y_exceeds = any(y > y_threshold for y in Y_values)\n",
    "\n",
    "    if not x_exceeds and not y_exceeds:\n",
    "        filtered_data.append(entry)\n",
    "    else:\n",
    "        removed_entries.append(entry)\n",
    "print(f\"x < 512 and y < 320\")\n",
    "for item in removed_entries:\n",
    "    print(f\"subject: {item.get('subject')} name: {item.get('name')} X: {item.get('X')} Y: {item.get('Y')}\")\n",
    "\n",
    "# Create the list of slides ['Slide1.jpg', 'Slide3.jpg', ..., 'Slide59.jpg']\n",
    "target_slides = [f'Slide{2*i-1}.jpg' for i in range(1, 31)]\n",
    "\n",
    "# Create a dictionary to track which slides each subject has\n",
    "subjects_slides = {subject: set() for subject in range(1, 12)}\n",
    "\n",
    "# Populate the dictionary with available slides for each subject\n",
    "for record in data:\n",
    "    subject = record['subject']\n",
    "    slide_name = record['name']\n",
    "    if subject in subjects_slides:\n",
    "        subjects_slides[subject].add(slide_name)\n",
    "\n",
    "# Now check for each subject which slides are missing\n",
    "for subject in range(1, 12):\n",
    "    available_slides = subjects_slides[subject]\n",
    "    missing_slides = [slide for slide in target_slides if slide not in available_slides]\n",
    "    \n",
    "    if missing_slides:\n",
    "        print(f\"Subject {subject} is missing the following slides: {', '.join(missing_slides)}\")\n",
    "    else:\n",
    "        print(f\"Subject {subject} has all the slides.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
